{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Analysis Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work by: GonÃ§alo Dias and Vicente Bandeira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves as both an implementation and a report for the work proposed to us in the CDLE cadeira in IACD. In this report, we present the findings of our comprehensive analysis of machine learning performance using various Python libraries for big data processing. As the scale and complexity of data continue to grow, the choice of tools and libraries becomes critical in ensuring efficient and effective machine learning workflows. To address this, we executed a full Machine Learning Pipeline using several prominent big data libraries: PySpark, Dask, Modin, JobLib, Rapids and Koalas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENT 1: Repeating the NYC taxi driver dataset study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.databricks.com/blog/2021/04/07/benchmark-koalas-pyspark-and-dask.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T09:52:17.981408Z",
     "start_time": "2024-06-03T09:52:17.975900Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T09:52:18.463310Z",
     "start_time": "2024-06-03T09:52:18.123271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 1.1.5\n",
      "numpy version: 1.19.5\n",
      "koalas version: 1.7.0\n",
      "dask version: 2021.04.1\n"
     ]
    }
   ],
   "source": [
    "import databricks.koalas as ks\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print('pandas version: %s' % pd.__version__)\n",
    "print('numpy version: %s' % np.__version__)\n",
    "print('koalas version: %s' % ks.__version__)\n",
    "print('dask version: %s' % dask.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"../yellow_tripdata_2011-01.parquet\"\n",
    "koalas_data = ks.read_parquet(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 13464997\n",
      "\n",
      "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  improvement_surcharge  total_amount  congestion_surcharge  airport_fee\n",
      "0         2  2011-01-01 00:10:00   2011-01-01 00:12:00                4            0.0           1               None           145           145             1          2.9    0.5      0.5        0.28           0.0                    0.0          4.18                   NaN          NaN\n",
      "1         2  2011-01-01 00:04:00   2011-01-01 00:13:00                4            0.0           1               None           264           264             1          5.7    0.5      0.5        0.24           0.0                    0.0          6.94                   NaN          NaN\n",
      "2         2  2011-01-01 00:14:00   2011-01-01 00:16:00                4            0.0           1               None           264           264             1          2.9    0.5      0.5        1.11           0.0                    0.0          5.01                   NaN          NaN\n",
      "3         2  2011-01-01 00:04:00   2011-01-01 00:06:00                5            0.0           1               None           146           146             1          2.9    0.5      0.5        0.00           0.0                    0.0          3.90                   NaN          NaN\n",
      "4         2  2011-01-01 00:08:00   2011-01-01 00:08:00                5            0.0           1               None           146           146             1          2.5    0.5      0.5        0.11           0.0                    0.0          3.61                   NaN          NaN\n",
      "5         2  2011-01-01 00:23:00   2011-01-01 00:23:00                1            0.0           1               None           146           146             2          2.5    0.5      0.5        0.00           0.0                    0.0          3.50                   NaN          NaN\n",
      "6         2  2011-01-01 00:25:00   2011-01-01 00:25:00                1            0.0           1               None           146           146             2          2.5    0.5      0.5        0.00           0.0                    0.0          3.50                   NaN          NaN\n",
      "7         1  2011-01-01 00:58:10   2011-01-01 01:15:35                1            8.0           1                  N           138           256             2         20.1    0.5      0.5        0.00           0.0                    0.0         21.10                   NaN          NaN\n",
      "8         1  2011-01-01 00:23:27   2011-01-01 00:39:39                1            1.6           1                  N           170           237             2          9.3    0.5      0.5        0.00           0.0                    0.0         10.30                   NaN          NaN\n",
      "9         1  2011-01-01 00:42:08   2011-01-01 00:51:50                4            2.5           1                  N           237           170             2          8.1    0.5      0.5        0.00           0.0                    0.0          9.10                   NaN          NaN\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows:\", len(koalas_data))\n",
    "print()\n",
    "print(koalas_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VendorID                          int64\n",
       "tpep_pickup_datetime     datetime64[ns]\n",
       "tpep_dropoff_datetime    datetime64[ns]\n",
       "passenger_count                   int64\n",
       "trip_distance                   float64\n",
       "RatecodeID                        int64\n",
       "store_and_fwd_flag               object\n",
       "PULocationID                      int64\n",
       "DOLocationID                      int64\n",
       "payment_type                      int64\n",
       "fare_amount                     float64\n",
       "extra                           float64\n",
       "mta_tax                         float64\n",
       "tip_amount                      float64\n",
       "tolls_amount                    float64\n",
       "improvement_surcharge           float64\n",
       "total_amount                    float64\n",
       "congestion_surcharge            float64\n",
       "airport_fee                     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koalas_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data is 35.84216914418919% of total data\n"
     ]
    }
   ],
   "source": [
    "expr_filter = (koalas_data['tip_amount'] >= 1) & (koalas_data['tip_amount'] <= 5)\n",
    " \n",
    "print(f'Filtered data is {len(koalas_data[expr_filter]) / len(koalas_data) * 100}% of total data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(f, df, benchmarks, name, **kwargs):\n",
    "    \"\"\"Benchmark the given function against the given DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f: function to benchmark\n",
    "    df: data frame\n",
    "    benchmarks: container for benchmark results\n",
    "    name: task name\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Duration (in seconds) of the given operation\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.monotonic()\n",
    "    ret = f(df, **kwargs)\n",
    "    benchmarks['duration'].append(time.monotonic() - start_time)\n",
    "    benchmarks['task'].append(name)\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]} seconds\")\n",
    "\n",
    "def get_results(benchmarks):\n",
    "    \"\"\"Return a pandas DataFrame containing benchmark results.\"\"\"\n",
    "    return pd.DataFrame.from_dict(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"../yellow_tripdata_201\"+str(i)+\"-01.parquet\" for i in range(1,4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Koalas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we utilized the Koalas library for our experiments, trying it on Standard operations, operations with filtering and operations with filtering and caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "koalas_data = ks.read_parquet(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "koalas_benchmarks = {\n",
    "    'duration': [],     # in seconds\n",
    "    'task': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return ks.read_parquet(\"../yellow_tripdata_2011-01.parquet\")\n",
    "\n",
    "def count(df=None):\n",
    "    return len(df)\n",
    "\n",
    "def count_index_length(df=None):\n",
    "    return len(df.index)\n",
    "\n",
    "def mean(df):\n",
    "    return df.fare_amount.mean()\n",
    "\n",
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std()\n",
    "\n",
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.tip_amount).mean()\n",
    "\n",
    "def sum_columns(df):\n",
    "    x = df.fare_amount + df.tip_amount\n",
    "    return x\n",
    "\n",
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.tip_amount).mean()\n",
    "\n",
    "def product_columns(df):\n",
    "    x = df.fare_amount * df.tip_amount\n",
    "    return x\n",
    "\n",
    "def value_counts(df):\n",
    "    val_counts = df.fare_amount.value_counts()\n",
    "    return val_counts\n",
    "\n",
    "\n",
    "#   In the original experiment, the following two functions used the longitude and latitude values of the pickup and the dropout places.\n",
    "#   Since the datasets provided by NYC TLC Trip Record Data no longer have longitude and latitude values (only the pickup and dropout places IDs),\n",
    "# we used arbitrary longitude and latitude values. The goal of this experiment is to compare the computational cost of the calculations, hence\n",
    "# the values are not relevant.\n",
    "def complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "    \n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2 - theta_1) / 2 * np.pi / 180) ** 2\n",
    "           + np.cos(theta_1 * np.pi / 180) * np.cos(theta_2 * np.pi / 180) * np.sin((phi_2 - phi_1) / 2 * np.pi / 180) ** 2)\n",
    "    ret = np.multiply(np.arctan2(np.sqrt(temp), np.sqrt(1-temp)),2)\n",
    "    return ret\n",
    "\n",
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "    \n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2 - theta_1) / 2 * np.pi / 180) ** 2\n",
    "           + np.cos(theta_1 * np.pi / 180) * np.cos(theta_2 * np.pi / 180) * np.sin((phi_2 - phi_1) / 2 * np.pi / 180) ** 2)\n",
    "    ret = np.multiply(np.arctan2(np.sqrt(temp), np.sqrt(1-temp)),2) \n",
    "    return ret.mean()\n",
    "\n",
    "def groupby_statistics(df):\n",
    "    gb = df.groupby(by='passenger_count').agg(\n",
    "      {\n",
    "        'fare_amount': ['mean', 'std'], \n",
    "        'tip_amount': ['mean', 'std']\n",
    "      }\n",
    "    )\n",
    "    return gb\n",
    "\n",
    "other = ks.DataFrame(groupby_statistics(koalas_data).to_pandas())\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "def join_count(df, other):\n",
    "    return len(df.merge(other.spark.hint(\"broadcast\"), left_index=True, right_index=True))\n",
    "\n",
    "def join_data(df, other):\n",
    "    ret = df.merge(other.spark.hint(\"broadcast\"), left_index=True, right_index=True)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file took: 0.25 seconds\n",
      "count took: 0.18699999997625127 seconds\n",
      "count index length took: 0.15600000001722947 seconds\n",
      "mean took: 0.48499999998603016 seconds\n",
      "standard deviation took: 0.5 seconds\n",
      "mean of columns addition took: 0.6560000000172295 seconds\n",
      "addition of columns took: 0.031000000017229468 seconds\n",
      "mean of columns multiplication took: 0.6099999999860302 seconds\n",
      "multiplication of columns took: 0.015000000013969839 seconds\n",
      "value counts took: 0.046999999962281436 seconds\n",
      "complex arithmetic ops took: 0.0 seconds\n",
      "mean of complex arithmetic ops took: 0.0 seconds\n",
      "groupby statistics took: 0.10899999999674037 seconds\n",
      "join count took: 21.45400000002701 seconds\n",
      "join took: 0.4529999999795109 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(read_file_parquet, df=None, benchmarks=koalas_benchmarks, name='read file')\n",
    "benchmark(count, df=koalas_data, benchmarks=koalas_benchmarks, name='count')\n",
    "benchmark(count_index_length, df=koalas_data, benchmarks=koalas_benchmarks, name='count index length')\n",
    "benchmark(mean, df=koalas_data, benchmarks=koalas_benchmarks, name='mean')\n",
    "benchmark(standard_deviation, df=koalas_data, benchmarks=koalas_benchmarks, name='standard deviation')\n",
    "benchmark(mean_of_sum, df=koalas_data, benchmarks=koalas_benchmarks, name='mean of columns addition')\n",
    "benchmark(sum_columns, df=koalas_data, benchmarks=koalas_benchmarks, name='addition of columns')\n",
    "benchmark(mean_of_product, df=koalas_data, benchmarks=koalas_benchmarks, name='mean of columns multiplication')\n",
    "benchmark(product_columns, df=koalas_data, benchmarks=koalas_benchmarks, name='multiplication of columns')\n",
    "benchmark(value_counts, df=koalas_data, benchmarks=koalas_benchmarks, name='value counts')\n",
    "benchmark(complicated_arithmetic_operation, df=koalas_data, benchmarks=koalas_benchmarks, name='complex arithmetic ops')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=koalas_data, benchmarks=koalas_benchmarks, name='mean of complex arithmetic ops')\n",
    "benchmark(groupby_statistics, df=koalas_data, benchmarks=koalas_benchmarks, name='groupby statistics')\n",
    "benchmark(join_count, koalas_data, benchmarks=koalas_benchmarks, name='join count', other=other)\n",
    "benchmark(join_data, koalas_data, benchmarks=koalas_benchmarks, name='join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations with filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_filter = (koalas_data.tip_amount >= 1) & (koalas_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "koalas_filtered = filter_data(koalas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 0.9689999999827705 seconds\n",
      "filtered count index length took: 0.9689999999827705 seconds\n",
      "filtered mean took: 0.8900000000139698 seconds\n",
      "filtered standard deviation took: 0.9220000000204891 seconds\n",
      "filtered mean of columns addition took: 0.875 seconds\n",
      "filtered addition of columns took: 0.030999999959021807 seconds\n",
      "filtered mean of columns multiplication took: 0.7970000000204891 seconds\n",
      "filtered multiplication of columns took: 0.03200000000651926 seconds\n",
      "filtered mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered complex arithmetic ops took: 0.0 seconds\n",
      "filtered value counts took: 0.045999999972991645 seconds\n",
      "filtered groupby statistics took: 0.0940000000409782 seconds\n",
      "filtered join took: 1.2810000000172295 seconds\n",
      "filtered join count took: 14.530999999959022 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered count')\n",
    "benchmark(count_index_length, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered count index length')\n",
    "benchmark(mean, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=koalas_filtered, benchmarks=koalas_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=koalas_filtered, benchmarks=koalas_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other = ks.DataFrame(groupby_statistics(koalas_filtered).to_pandas())\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_data, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered join', other=other)\n",
    "benchmark(join_count, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered join count', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations with filtering and caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enforce caching: 4826147 rows of filtered data\n"
     ]
    }
   ],
   "source": [
    "koalas_filtered_cached = koalas_filtered.spark.cache()\n",
    "print(f'Enforce caching: {len(koalas_filtered_cached)} rows of filtered data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered and cached count took: 1.3899999999557622 seconds\n",
      "filtered and cached count index length took: 1.422000000020489 seconds\n",
      "filtered and cached mean took: 1.2820000000065193 seconds\n",
      "filtered and cached standard deviation took: 1.0929999999934807 seconds\n",
      "filtered and cached mean of columns addition took: 1.922000000020489 seconds\n",
      "filtered and cached addition of columns took: 0.09399999998277053 seconds\n",
      "filtered and cached mean of columns multiplication took: 2.047000000020489 seconds\n",
      "filtered and cached multiplication of columns took: 0.030999999959021807 seconds\n",
      "filtered and cached mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered and cached complex arithmetic ops took: 0.0 seconds\n",
      "filtered and cached value counts took: 0.07800000003771856 seconds\n",
      "filtered and cached groupby statistics took: 0.125 seconds\n",
      "filtered and cached join took: 0.5939999999827705 seconds\n",
      "filtered and cached join count took: 11.547000000020489 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached count')\n",
    "benchmark(count_index_length, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached count index length')\n",
    "benchmark(mean, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached mean')\n",
    "benchmark(standard_deviation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached standard deviation')\n",
    "benchmark(mean_of_sum, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered and cached mean of columns addition')\n",
    "benchmark(sum_columns, df=koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached addition of columns')\n",
    "benchmark(mean_of_product, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered and cached mean of columns multiplication')\n",
    "benchmark(product_columns, df=koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached complex arithmetic ops')\n",
    "benchmark(value_counts, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered and cached value counts')\n",
    "benchmark(groupby_statistics, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached groupby statistics')\n",
    "\n",
    "other = ks.DataFrame(groupby_statistics(koalas_filtered).to_pandas())\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_data, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached join', other=other)\n",
    "benchmark(join_count, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached join count', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeated this experiment with Dask, on the same 3 sets of operations: standard, with filtering and with filtering and caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gapmd\\anaconda3\\envs\\big-data\\lib\\site-packages\\distributed\\node.py:151: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 61380 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cluster = LocalCluster(memory_limit='8GB')\n",
    "client = Client(cluster)\n",
    "\n",
    "dask_data = dd.read_parquet(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_benchmarks = {\n",
    "    'duration': [],  # in seconds\n",
    "    'task': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return dd.read_parquet(\"../yellow_tripdata_2011-01.parquet\")\n",
    "\n",
    "def count(df=None):\n",
    "    return len(df)\n",
    "\n",
    "def count_index_length(df=None):\n",
    "    return len(df.index)\n",
    "\n",
    "def mean(df):\n",
    "    return df.fare_amount.mean().compute()\n",
    "\n",
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std().compute()\n",
    "\n",
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.tip_amount).mean().compute()\n",
    "\n",
    "def sum_columns(df):\n",
    "    return (df.fare_amount + df.tip_amount).compute()\n",
    "\n",
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.tip_amount).mean().compute()\n",
    "\n",
    "def product_columns(df):\n",
    "    return (df.fare_amount * df.tip_amount).compute()\n",
    "\n",
    "def value_counts(df):\n",
    "    return df.fare_amount.value_counts().compute()\n",
    "\n",
    "#   In the original experiment, the following two functions used the longitude and latitude values of the pickup and the dropout places.\n",
    "#   Since the datasets provided by NYC TLC Trip Record Data no longer have longitude and latitude values (only the pickup and dropout places IDs),\n",
    "# we used arbitrary longitude and latitude values. The goal of this experiment is to compare the computational cost of the calculations, hence\n",
    "# the values are not relevant.\n",
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret.mean()\n",
    "\n",
    "def complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret\n",
    "\n",
    "def groupby_statistics(df):\n",
    "    return df.groupby(by='passenger_count').agg({\n",
    "        'fare_amount': ['mean', 'std'],\n",
    "        'tip_amount': ['mean', 'std'] \n",
    "    })\n",
    "\n",
    "other = groupby_statistics(dask_data)\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "def join_count(df, other):\n",
    "    return len(dd.merge(df, other, left_index=True, right_index=True))\n",
    "\n",
    "def join_data(df, other):\n",
    "    return dd.merge(df, other, left_index=True, right_index=True).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file took: 0.030999999959021807 seconds\n",
      "count took: 2.1880000000237487 seconds\n",
      "count index length took: 48.20299999997951 seconds\n",
      "mean took: 0.9370000000344589 seconds\n",
      "standard deviation took: 2.702999999979511 seconds\n",
      "mean of columns addition took: 2.327999999979511 seconds\n",
      "addition of columns took: 1.9540000000270084 seconds\n",
      "mean of columns multiplication took: 1.375 seconds\n",
      "multiplication of columns took: 1.5619999999762513 seconds\n",
      "value counts took: 0.6880000000237487 seconds\n",
      "mean of complex arithmetic ops took: 0.0 seconds\n",
      "complex arithmetic ops took: 0.0 seconds\n",
      "groupby statistics took: 0.031000000017229468 seconds\n",
      "join count took: 19.71899999998277 seconds\n",
      "join took: 14.795999999972992 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(read_file_parquet, df=None, benchmarks=dask_benchmarks, name='read file')\n",
    "benchmark(count, df=dask_data, benchmarks=dask_benchmarks, name='count')\n",
    "benchmark(count_index_length, df=dask_data, benchmarks=dask_benchmarks, name='count index length')\n",
    "benchmark(mean, df=dask_data, benchmarks=dask_benchmarks, name='mean')\n",
    "benchmark(standard_deviation, df=dask_data, benchmarks=dask_benchmarks, name='standard deviation')\n",
    "benchmark(mean_of_sum, df=dask_data, benchmarks=dask_benchmarks, name='mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_data, benchmarks=dask_benchmarks, name='addition of columns')\n",
    "benchmark(mean_of_product, df=dask_data, benchmarks=dask_benchmarks, name='mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_data, benchmarks=dask_benchmarks, name='multiplication of columns')\n",
    "benchmark(value_counts, df=dask_data, benchmarks=dask_benchmarks, name='value counts')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=dask_data, benchmarks=dask_benchmarks, name='mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, df=dask_data, benchmarks=dask_benchmarks, name='complex arithmetic ops')\n",
    "benchmark(groupby_statistics, df=dask_data, benchmarks=dask_benchmarks, name='groupby statistics')\n",
    "benchmark(join_count, dask_data, benchmarks=dask_benchmarks, name='join count', other=other)\n",
    "benchmark(join_data, dask_data, benchmarks=dask_benchmarks, name='join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations with filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_filter = (dask_data.tip_amount >= 1) & (dask_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "dask_filtered = filter_data(dask_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 14.85899999999674 seconds\n",
      "filtered count index length took: 12.311999999976251 seconds\n",
      "filtered mean took: 13.688000000023749 seconds\n",
      "filtered standard deviation took: 11.375 seconds\n",
      "filtered mean of columns addition took: 14.10899999999674 seconds\n",
      "filtered addition of columns took: 11.297000000020489 seconds\n",
      "filtered mean of columns multiplication took: 13.875 seconds\n",
      "filtered multiplication of columns took: 12.84399999998277 seconds\n",
      "filtered mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered complex arithmetic ops took: 0.0 seconds\n",
      "filtered value counts took: 18.34299999999348 seconds\n",
      "filtered groupby statistics took: 0.2029999999795109 seconds\n",
      "filtered join count took: 11.51600000000326 seconds\n",
      "filtered join took: 12.936999999976251 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, dask_filtered, benchmarks=dask_benchmarks, name='filtered count')\n",
    "benchmark(count_index_length, dask_filtered, benchmarks=dask_benchmarks, name='filtered count index length')\n",
    "benchmark(mean, dask_filtered, benchmarks=dask_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, dask_filtered, benchmarks=dask_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, dask_filtered, benchmarks=dask_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_filtered, benchmarks=dask_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, dask_filtered, benchmarks=dask_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_filtered, benchmarks=dask_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, dask_filtered, benchmarks=dask_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, dask_filtered, benchmarks=dask_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, dask_filtered, benchmarks=dask_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, dask_filtered, benchmarks=dask_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other = groupby_statistics(dask_filtered)\n",
    "other.columns = pd.Index([e[0] +'_'+ e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_count, dask_filtered, benchmarks=dask_benchmarks, name='filtered join count', other=other)\n",
    "benchmark(join_data, dask_filtered, benchmarks=dask_benchmarks, name='filtered join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations with filtering and caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting until all futures are finished\n",
      "All futures are finished\n"
     ]
    }
   ],
   "source": [
    "dask_filtered = client.persist(dask_filtered)\n",
    "\n",
    "from distributed import wait\n",
    "print('Waiting until all futures are finished')\n",
    "wait(dask_filtered)\n",
    "print('All futures are finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered and cached count took: 0.01600000000325963 seconds\n",
      "filtered and cached count index length took: 0.031000000017229468 seconds\n",
      "filtered and cached mean took: 0.10999999998603016 seconds\n",
      "filtered and cached standard deviation took: 0.21799999999348074 seconds\n",
      "filtered and cached mean of columns addition took: 0.10999999998603016 seconds\n",
      "filtered and cached addition of columns took: 0.5929999999934807 seconds\n",
      "filtered and cached mean of columns multiplication took: 0.10899999999674037 seconds\n",
      "filtered and cached multiplication of columns took: 0.5160000000032596 seconds\n",
      "filtered and cached mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered and cached complex arithmetic ops took: 0.0 seconds\n",
      "filtered and cached value counts took: 0.09299999999348074 seconds\n",
      "filtered and cached groupby statistics took: 0.01600000000325963 seconds\n",
      "filtered and cached join count took: 0.7650000000139698 seconds\n",
      "filtered and cached join took: 1.5320000000065193 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached count')\n",
    "benchmark(count_index_length, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached count index length')\n",
    "benchmark(mean, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached mean')\n",
    "benchmark(standard_deviation, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached standard deviation')\n",
    "benchmark(mean_of_sum, dask_filtered, benchmarks=dask_benchmarks, name ='filtered and cached mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached addition of columns')\n",
    "benchmark(mean_of_product, dask_filtered, benchmarks=dask_benchmarks, name ='filtered and cached mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached complex arithmetic ops')\n",
    "benchmark(value_counts, dask_filtered, benchmarks=dask_benchmarks, name ='filtered and cached value counts')\n",
    "benchmark(groupby_statistics, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached groupby statistics')\n",
    "\n",
    "other = groupby_statistics(dask_filtered)\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_count, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached join count', other=other)\n",
    "benchmark(join_data, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached join', other=other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:61381</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:61380/status' target='_blank'>http://127.0.0.1:61380/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>29.80 GiB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:61381' processes=3 threads=6, memory=22.35 GiB>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KOALAS\n",
      "\n",
      "['read file', 'count', 'count index length', 'mean', 'standard deviation', 'mean of columns addition', 'addition of columns', 'mean of columns multiplication', 'multiplication of columns', 'value counts', 'complex arithmetic ops', 'mean of complex arithmetic ops', 'groupby statistics', 'join count', 'join', 'filtered count', 'filtered count index length', 'filtered mean', 'filtered standard deviation', 'filtered mean of columns addition', 'filtered addition of columns', 'filtered mean of columns multiplication', 'filtered multiplication of columns', 'filtered mean of complex arithmetic ops', 'filtered complex arithmetic ops', 'filtered value counts', 'filtered groupby statistics', 'filtered join', 'filtered join count', 'filtered and cached count', 'filtered and cached count index length', 'filtered and cached mean', 'filtered and cached standard deviation', 'filtered and cached mean of columns addition', 'filtered and cached addition of columns', 'filtered and cached mean of columns multiplication', 'filtered and cached multiplication of columns', 'filtered and cached mean of complex arithmetic ops', 'filtered and cached complex arithmetic ops', 'filtered and cached value counts', 'filtered and cached groupby statistics', 'filtered and cached join', 'filtered and cached join count']\n",
      "[0.25, 0.18699999997625127, 0.15600000001722947, 0.48499999998603016, 0.5, 0.6560000000172295, 0.031000000017229468, 0.6099999999860302, 0.015000000013969839, 0.046999999962281436, 0.0, 0.0, 0.10899999999674037, 21.45400000002701, 0.4529999999795109, 0.9689999999827705, 0.9689999999827705, 0.8900000000139698, 0.9220000000204891, 0.875, 0.030999999959021807, 0.7970000000204891, 0.03200000000651926, 0.0, 0.0, 0.045999999972991645, 0.0940000000409782, 1.2810000000172295, 14.530999999959022, 1.3899999999557622, 1.422000000020489, 1.2820000000065193, 1.0929999999934807, 1.922000000020489, 0.09399999998277053, 2.047000000020489, 0.030999999959021807, 0.0, 0.0, 0.07800000003771856, 0.125, 0.5939999999827705, 11.547000000020489]\n",
      "\n",
      "----------------------------------------------------------------------------------\n",
      "\n",
      "['read file', 'count', 'count index length', 'mean', 'standard deviation', 'mean of columns addition', 'addition of columns', 'mean of columns multiplication', 'multiplication of columns', 'value counts', 'mean of complex arithmetic ops', 'complex arithmetic ops', 'groupby statistics', 'join count', 'join', 'filtered count', 'filtered count index length', 'filtered mean', 'filtered standard deviation', 'filtered mean of columns addition', 'filtered addition of columns', 'filtered mean of columns multiplication', 'filtered multiplication of columns', 'filtered mean of complex arithmetic ops', 'filtered complex arithmetic ops', 'filtered value counts', 'filtered groupby statistics', 'filtered join count', 'filtered join', 'filtered and cached count', 'filtered and cached count index length', 'filtered and cached mean', 'filtered and cached standard deviation', 'filtered and cached mean of columns addition', 'filtered and cached addition of columns', 'filtered and cached mean of columns multiplication', 'filtered and cached multiplication of columns', 'filtered and cached mean of complex arithmetic ops', 'filtered and cached complex arithmetic ops', 'filtered and cached value counts', 'filtered and cached groupby statistics', 'filtered and cached join count', 'filtered and cached join']\n",
      "[0.030999999959021807, 2.1880000000237487, 48.20299999997951, 0.9370000000344589, 2.702999999979511, 2.327999999979511, 1.9540000000270084, 1.375, 1.5619999999762513, 0.6880000000237487, 0.0, 0.0, 0.031000000017229468, 19.71899999998277, 14.795999999972992, 14.85899999999674, 12.311999999976251, 13.688000000023749, 11.375, 14.10899999999674, 11.297000000020489, 13.875, 12.84399999998277, 0.0, 0.0, 18.34299999999348, 0.2029999999795109, 11.51600000000326, 12.936999999976251, 0.01600000000325963, 0.031000000017229468, 0.10999999998603016, 0.21799999999348074, 0.10999999998603016, 0.5929999999934807, 0.10899999999674037, 0.5160000000032596, 0.0, 0.0, 0.09299999999348074, 0.01600000000325963, 0.7650000000139698, 1.5320000000065193]\n"
     ]
    }
   ],
   "source": [
    "print(\"KOALAS\\n\")\n",
    "print(koalas_benchmarks)\n",
    "print(koalas_benchmarks['task'])\n",
    "print(koalas_benchmarks['duration'])\n",
    "print(\"\\n----------------------------------------------------------------------------------\\n\")\n",
    "print(\"DASK\")\n",
    "print(dask_benchmarks)\n",
    "print(dask_benchmarks['task'])\n",
    "print(dask_benchmarks['duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task read file: Dask performs 8.064516139692557x better than Koalas\n",
      "Task count: Koalas performs 11.700534760971237x better than Dask\n",
      "Task count index length: Koalas performs 308.9935897093315x better than Dask\n",
      "Task mean: Koalas performs 1.931958763013295x better than Dask\n",
      "Task standard deviation: Koalas performs 5.405999999959022x better than Dask\n",
      "Task mean of columns addition: Koalas performs 3.548780487680438x better than Dask\n",
      "Task addition of columns: Koalas performs 63.03225803035471x better than Dask\n",
      "Task mean of columns multiplication: Koalas performs 2.2540983607073595x better than Dask\n",
      "Task multiplication of columns: Koalas performs 104.13333323476836x better than Dask\n",
      "Task value counts: Koalas performs 14.638297884593284x better than Dask\n",
      "Task mean of complex arithmetic ops: both Dask and Koalas ran this task in less time than the measurable threshold.\n",
      "Task complex arithmetic ops: both Dask and Koalas ran this task in less time than the measurable threshold.\n",
      "Task groupby statistics: Dask performs 3.5161290301986887x better than Koalas\n",
      "Task join count: Dask performs 1.087986206199389x better than Koalas\n",
      "Task join: Koalas performs 32.66225165704682x better than Dask\n",
      "Task filtered count: Koalas performs 15.33436532534669x better than Dask\n",
      "Task filtered count index length: Koalas performs 12.705882353142586x better than Dask\n",
      "Task filtered mean: Koalas performs 15.379775280684152x better than Dask\n",
      "Task filtered standard deviation: Koalas performs 12.3373101949536x better than Dask\n",
      "Task filtered mean of columns addition: Koalas performs 16.1245714285677x better than Dask\n",
      "Task filtered addition of columns: Koalas performs 364.41935532108823x better than Dask\n",
      "Task filtered mean of columns multiplication: Koalas performs 17.409033876591348x better than Dask\n",
      "Task filtered multiplication of columns: Koalas performs 401.37499991769073x better than Dask\n",
      "Task filtered mean of complex arithmetic ops: both Dask and Koalas ran this task in less time than the measurable threshold.\n",
      "Task filtered complex arithmetic ops: both Dask and Koalas ran this task in less time than the measurable threshold.\n",
      "Task filtered value counts: Koalas performs 398.76086979920336x better than Dask\n",
      "Task filtered groupby statistics: Koalas performs 2.159574466925696x better than Dask\n",
      "Task filtered join count: Koalas performs 8.9898516782579x better than Dask\n",
      "Task filtered join: Dask performs 1.1232124913029062x better than Koalas\n",
      "Task filtered and cached count: Dask performs 86.87499997953637x better than Koalas\n",
      "Task filtered and cached count index length: Dask performs 45.870967717101834x better than Koalas\n",
      "Task filtered and cached mean: Dask performs 11.654545456084831x better than Koalas\n",
      "Task filtered and cached standard deviation: Dask performs 5.013761468009939x better than Koalas\n",
      "Task filtered and cached mean of columns addition: Dask performs 17.47272727513255x better than Koalas\n",
      "Task filtered and cached addition of columns: Koalas performs 6.30851063938482x better than Dask\n",
      "Task filtered and cached mean of columns multiplication: Dask performs 18.77981651451105x better than Koalas\n",
      "Task filtered and cached multiplication of columns: Koalas performs 16.645161312430588x better than Dask\n",
      "Task filtered and cached mean of complex arithmetic ops: both Dask and Koalas ran this task in less time than the measurable threshold.\n",
      "Task filtered and cached complex arithmetic ops: both Dask and Koalas ran this task in less time than the measurable threshold.\n",
      "Task filtered and cached value counts: Koalas performs 1.1923076916475464x better than Dask\n",
      "Task filtered and cached groupby statistics: Dask performs 7.812499998408384x better than Koalas\n",
      "Task filtered and cached join count: Koalas performs 1.287878787939662x better than Dask\n",
      "Task filtered and cached join: Dask performs 7.5372062662998385x better than Koalas\n"
     ]
    }
   ],
   "source": [
    "dask_koalas_duration_ratio = []\n",
    "i=0\n",
    "for task in dask_benchmarks['task']:\n",
    "    dask_duration_i = dask_benchmarks['duration'][i]\n",
    "    koalas_duration_i = koalas_benchmarks['duration'][i]\n",
    "    if dask_duration_i == 0 and koalas_duration_i == 0:\n",
    "        print(f\"Task {task}: both Dask and Koalas ran this task in less time than the measurable threshold.\")\n",
    "        i+=1\n",
    "        continue\n",
    "    if dask_duration_i == 0:\n",
    "        print(f\"Task {task}: Dask ran this task in less time than the measurable threshold while Koalas took {koalas_duration_i} seconds\")\n",
    "        i+=1\n",
    "        continue\n",
    "    if koalas_duration_i == 0:       \n",
    "        print(f\"Task {task}: Koalas ran this task in less time than the measurable threshold while Dask took {dask_duration_i} seconds\")\n",
    "        i+=1\n",
    "        continue\n",
    "    ratio = dask_duration_i / koalas_duration_i\n",
    "    dask_koalas_duration_ratio.append(ratio)\n",
    "    if ratio >= 1:\n",
    "        print(f\"Task {task}: Koalas performs {ratio}x better than Dask\")\n",
    "    else:\n",
    "        print(f\"Task {task}: Dask performs {1/ratio}x better than Koalas\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Python can only measure time with a precision of around 10^-6 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENT 2: Running datasets with different combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask + Modin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modin.pandas as mpd\n",
    "from modin.config import Engine\n",
    "\n",
    "print('modin version: %s' % mpd.__version__)\n",
    "\n",
    "Engine.put(\"dask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_modin_data = mpd.read_parquet(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_modin_benchmarks = {\n",
    "    'duration': [],     # in seconds\n",
    "    'task': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return mpd.read_parquet(\"../yellow_tripdata_2011-01.parquet\")\n",
    "\n",
    "def count(df=None):\n",
    "    return len(df)\n",
    "\n",
    "def count_index_length(df=None):\n",
    "    return len(df.index)\n",
    "\n",
    "def mean(df):\n",
    "    return df.fare_amount.mean()\n",
    "\n",
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std()\n",
    "\n",
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.tip_amount).mean()\n",
    "\n",
    "def sum_columns(df):\n",
    "    x = df.fare_amount + df.tip_amount\n",
    "    return x\n",
    "\n",
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.tip_amount).mean()\n",
    "\n",
    "def product_columns(df):\n",
    "    x = df.fare_amount * df.tip_amount\n",
    "    return x\n",
    "\n",
    "def value_counts(df):\n",
    "    val_counts = df.fare_amount.value_counts()\n",
    "    return val_counts\n",
    "\n",
    "\n",
    "#   In the original experiment, the following two functions used the longitude and latitude values of the pickup and the dropout places.\n",
    "#   Since the datasets provided by NYC TLC Trip Record Data no longer have longitude and latitude values (only the pickup and dropout places IDs),\n",
    "# we used arbitrary longitude and latitude values. The goal of this experiment is to compare the computational cost of the calculations, hence\n",
    "# the values are not relevant.\n",
    "def complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "    \n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2 - theta_1) / 2 * np.pi / 180) ** 2\n",
    "           + np.cos(theta_1 * np.pi / 180) * np.cos(theta_2 * np.pi / 180) * np.sin((phi_2 - phi_1) / 2 * np.pi / 180) ** 2)\n",
    "    ret = np.multiply(np.arctan2(np.sqrt(temp), np.sqrt(1-temp)),2)\n",
    "    return ret\n",
    "\n",
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "    \n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2 - theta_1) / 2 * np.pi / 180) ** 2\n",
    "           + np.cos(theta_1 * np.pi / 180) * np.cos(theta_2 * np.pi / 180) * np.sin((phi_2 - phi_1) / 2 * np.pi / 180) ** 2)\n",
    "    ret = np.multiply(np.arctan2(np.sqrt(temp), np.sqrt(1-temp)),2) \n",
    "    return ret.mean()\n",
    "\n",
    "def groupby_statistics(df):\n",
    "    gb = df.groupby(by='passenger_count').agg(\n",
    "      {\n",
    "        'fare_amount': ['mean', 'std'], \n",
    "        'tip_amount': ['mean', 'std']\n",
    "      }\n",
    "    )\n",
    "    return gb\n",
    "\n",
    "other = groupby_statistics(dask_modin_data)\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "def join_count(df, other):\n",
    "    return len(mpd.merge(df, other, left_index=True, right_index=True))\n",
    "\n",
    "def join_data(df, other):\n",
    "    return mpd.merge(df, other, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(read_file_parquet, df=None, benchmarks=dask_modin_benchmarks, name='read file')\n",
    "benchmark(count, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='count')\n",
    "benchmark(count_index_length, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='count index length')\n",
    "benchmark(mean, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='mean')\n",
    "benchmark(standard_deviation, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='standard deviation')\n",
    "benchmark(mean_of_sum, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='addition of columns')\n",
    "benchmark(mean_of_product, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='multiplication of columns')\n",
    "benchmark(value_counts, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='value counts')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='complex arithmetic ops')\n",
    "benchmark(groupby_statistics, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='groupby statistics')\n",
    "benchmark(join_count, dask_modin_data, benchmarks=dask_modin_benchmarks, name='join count', other=other)\n",
    "benchmark(join_data, dask_modin_data, benchmarks=dask_modin_benchmarks, name='join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations with filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_filter = (dask_modin_data.tip_amount >= 1) & (dask_modin_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "dask_modin_filtered = filter_data(dask_modin_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dask_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\n",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n",
      "\n",
      "\u001b[1;32m----> 1\u001b[0m benchmark(count, \u001b[43mdask_filtered\u001b[49m, benchmarks\u001b[38;5;241m=\u001b[39mdask_benchmarks, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiltered count\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\n",
      "\u001b[0;32m      2\u001b[0m benchmark(count_index_length, dask_filtered, benchmarks\u001b[38;5;241m=\u001b[39mdask_benchmarks, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiltered count index length\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\n",
      "\u001b[0;32m      3\u001b[0m benchmark(mean, dask_filtered, benchmarks\u001b[38;5;241m=\u001b[39mdask_benchmarks, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiltered mean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dask_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "benchmark(count, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered count')\n",
    "benchmark(count_index_length, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered count index length')\n",
    "benchmark(mean, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other = groupby_statistics(dask_modin_filtered)\n",
    "other.columns = pd.Index([e[0] +'_'+ e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_count, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered join count', other=other)\n",
    "benchmark(join_data, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations with filtering and caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_modin_filtered = client.persist(dask_modin_filtered)\n",
    "\n",
    "from distributed import wait\n",
    "print('Waiting until all futures are finished')\n",
    "wait(dask_modin_filtered)\n",
    "print('All futures are finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dask_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\n",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n",
      "\n",
      "\u001b[1;32m----> 1\u001b[0m benchmark(count, \u001b[43mdask_filtered\u001b[49m, benchmarks\u001b[38;5;241m=\u001b[39mdask_benchmarks, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiltered count\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\n",
      "\u001b[0;32m      2\u001b[0m benchmark(count_index_length, dask_filtered, benchmarks\u001b[38;5;241m=\u001b[39mdask_benchmarks, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiltered count index length\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\n",
      "\u001b[0;32m      3\u001b[0m benchmark(mean, dask_filtered, benchmarks\u001b[38;5;241m=\u001b[39mdask_benchmarks, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiltered mean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dask_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "benchmark(count, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered count')\n",
    "benchmark(count_index_length, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered count index length')\n",
    "benchmark(mean, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other = groupby_statistics(dask_modin_filtered)\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_count, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered join count', other=other)\n",
    "benchmark(join_data, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered join', other=other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\n",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n",
      "\n",
      "\u001b[1;32m----> 1\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241m.\u001b[39mrestart()\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
