{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeating the NYC taxi driver dataset study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.databricks.com/blog/2021/04/07/benchmark-koalas-pyspark-and-dask.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 1.1.5\n",
      "numpy version: 1.19.5\n",
      "koalas version: 1.7.0\n",
      "dask version: 2021.04.1\n"
     ]
    }
   ],
   "source": [
    "import databricks.koalas as ks\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print('pandas version: %s' % pd.__version__)\n",
    "print('numpy version: %s' % np.__version__)\n",
    "print('koalas version: %s' % ks.__version__)\n",
    "print('dask version: %s' % dask.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"../yellow_tripdata_2011-01.parquet\"\n",
    "koalas_data = ks.read_parquet(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 13464997\n",
      "\n",
      "    VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  improvement_surcharge  total_amount  congestion_surcharge  airport_fee\n",
      "0          2  2011-01-01 00:10:00   2011-01-01 00:12:00                4            0.0           1               None           145           145             1          2.9    0.5      0.5        0.28           0.0                    0.0          4.18                   NaN          NaN\n",
      "1          2  2011-01-01 00:04:00   2011-01-01 00:13:00                4            0.0           1               None           264           264             1          5.7    0.5      0.5        0.24           0.0                    0.0          6.94                   NaN          NaN\n",
      "2          2  2011-01-01 00:14:00   2011-01-01 00:16:00                4            0.0           1               None           264           264             1          2.9    0.5      0.5        1.11           0.0                    0.0          5.01                   NaN          NaN\n",
      "3          2  2011-01-01 00:04:00   2011-01-01 00:06:00                5            0.0           1               None           146           146             1          2.9    0.5      0.5        0.00           0.0                    0.0          3.90                   NaN          NaN\n",
      "4          2  2011-01-01 00:08:00   2011-01-01 00:08:00                5            0.0           1               None           146           146             1          2.5    0.5      0.5        0.11           0.0                    0.0          3.61                   NaN          NaN\n",
      "5          2  2011-01-01 00:23:00   2011-01-01 00:23:00                1            0.0           1               None           146           146             2          2.5    0.5      0.5        0.00           0.0                    0.0          3.50                   NaN          NaN\n",
      "6          2  2011-01-01 00:25:00   2011-01-01 00:25:00                1            0.0           1               None           146           146             2          2.5    0.5      0.5        0.00           0.0                    0.0          3.50                   NaN          NaN\n",
      "7          1  2011-01-01 00:58:10   2011-01-01 01:15:35                1            8.0           1                  N           138           256             2         20.1    0.5      0.5        0.00           0.0                    0.0         21.10                   NaN          NaN\n",
      "8          1  2011-01-01 00:23:27   2011-01-01 00:39:39                1            1.6           1                  N           170           237             2          9.3    0.5      0.5        0.00           0.0                    0.0         10.30                   NaN          NaN\n",
      "9          1  2011-01-01 00:42:08   2011-01-01 00:51:50                4            2.5           1                  N           237           170             2          8.1    0.5      0.5        0.00           0.0                    0.0          9.10                   NaN          NaN\n",
      "10         1  2011-01-01 00:53:36   2011-01-01 01:17:43                2            3.9           1                  N           170           239             1         14.9    0.5      0.5        2.38           0.0                    0.0         18.28                   NaN          NaN\n",
      "11         1  2011-01-01 00:37:47   2011-01-01 00:41:20                2            0.6           1                  N            90            90             2          4.1    0.5      0.5        0.00           0.0                    0.0          5.10                   NaN          NaN\n",
      "12         1  2011-01-01 00:42:49   2011-01-01 00:52:00                4            0.9           1                  N            90           186             2          6.5    0.5      0.5        0.00           0.0                    0.0          7.50                   NaN          NaN\n",
      "13         1  2011-01-01 00:56:28   2011-01-01 01:22:36                1            3.9           1                  Y            90           238             2         15.3    0.5      0.5        0.00           0.0                    0.0         16.30                   NaN          NaN\n",
      "14         1  2011-01-01 00:11:22   2011-01-01 00:14:36                2            0.7           1                  N           113            79             2          4.1    0.0      0.5        0.00           0.0                    0.0          4.60                   NaN          NaN\n",
      "15         1  2011-01-01 00:16:43   2011-01-01 00:24:39                2            1.9           1                  N            79           170             1          6.9    0.0      0.5        1.00           0.0                    0.0          8.40                   NaN          NaN\n",
      "16         1  2011-01-01 00:32:25   2011-01-01 00:48:46                2            3.3           1                  N           170           142             2         11.3    0.0      0.5        0.00           0.0                    0.0         11.80                   NaN          NaN\n",
      "17         1  2011-01-01 00:50:52   2011-01-01 01:25:47                2            5.3           1                  Y           142           112             2         20.1    0.0      0.5        0.00           4.8                    0.0         25.40                   NaN          NaN\n",
      "18         1  2011-01-01 00:20:22   2011-01-01 00:26:13                1            1.0           4                  N           114           249             2          5.3    0.5      0.5        0.00           0.0                    0.0          6.30                   NaN          NaN\n",
      "19         1  2011-01-01 00:28:45   2011-01-01 00:46:14                1            4.3           4                  N           249           143             2         13.7    0.5      0.5        0.00           0.0                    0.0         14.70                   NaN          NaN\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows:\", len(koalas_data))\n",
    "print()\n",
    "print(koalas_data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VendorID                          int64\n",
       "tpep_pickup_datetime     datetime64[ns]\n",
       "tpep_dropoff_datetime    datetime64[ns]\n",
       "passenger_count                   int64\n",
       "trip_distance                   float64\n",
       "RatecodeID                        int64\n",
       "store_and_fwd_flag               object\n",
       "PULocationID                      int64\n",
       "DOLocationID                      int64\n",
       "payment_type                      int64\n",
       "fare_amount                     float64\n",
       "extra                           float64\n",
       "mta_tax                         float64\n",
       "tip_amount                      float64\n",
       "tolls_amount                    float64\n",
       "improvement_surcharge           float64\n",
       "total_amount                    float64\n",
       "congestion_surcharge            float64\n",
       "airport_fee                     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koalas_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data is 35.84216914418919% of total data\n"
     ]
    }
   ],
   "source": [
    "expr_filter = (koalas_data['tip_amount'] >= 1) & (koalas_data['tip_amount'] <= 5)\n",
    " \n",
    "print(f'Filtered data is {len(koalas_data[expr_filter]) / len(koalas_data) * 100}% of total data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(f, df, benchmarks, name, **kwargs):\n",
    "    \"\"\"Benchmark the given function against the given DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f: function to benchmark\n",
    "    df: data frame\n",
    "    benchmarks: container for benchmark results\n",
    "    name: task name\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Duration (in seconds) of the given operation\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    ret = f(df, **kwargs)\n",
    "    benchmarks['duration'].append(time.time() - start_time)\n",
    "    benchmarks['task'].append(name)\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]} seconds\")\n",
    "\n",
    "def get_results(benchmarks):\n",
    "    \"\"\"Return a pandas DataFrame containing benchmark results.\"\"\"\n",
    "    return pd.DataFrame.from_dict(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"../yellow_tripdata_201\"+str(i)+\"-01.parquet\" for i in range(1,4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Koalas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "koalas_data = ks.read_parquet(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "koalas_benchmarks = {\n",
    "    'duration': [],     # in seconds\n",
    "    'task': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return ks.read_parquet(\"../yellow_tripdata_2011-01.parquet\")\n",
    "\n",
    "def count(df=None):\n",
    "    return len(df)\n",
    "\n",
    "def count_index_length(df=None):\n",
    "    return len(df.index)\n",
    "\n",
    "def mean(df):\n",
    "    return df.fare_amount.mean()\n",
    "\n",
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std()\n",
    "\n",
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.tip_amount).mean()\n",
    "\n",
    "def sum_columns(df):\n",
    "    x = df.fare_amount + df.tip_amount\n",
    "    return x\n",
    "\n",
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.tip_amount).mean()\n",
    "\n",
    "def product_columns(df):\n",
    "    x = df.fare_amount * df.tip_amount\n",
    "    return x\n",
    "\n",
    "def value_counts(df):\n",
    "    val_counts = df.fare_amount.value_counts()\n",
    "    return val_counts\n",
    "\n",
    "\n",
    "#   In the original experiment, the following two functions used the longitude and latitude values of the pickup and the dropout places.\n",
    "#   Since the datasets provided by NYC TLC Trip Record Data no longer have longitude and latitude values (only the pickup and dropout places IDs),\n",
    "# we used arbitrary longitude and latitude values. The goal of this experiment is to compare the computational cost of the calculations, hence\n",
    "# the values are not relevant.\n",
    "def complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "    \n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2 - theta_1) / 2 * np.pi / 180) ** 2\n",
    "           + np.cos(theta_1 * np.pi / 180) * np.cos(theta_2 * np.pi / 180) * np.sin((phi_2 - phi_1) / 2 * np.pi / 180) ** 2)\n",
    "    ret = np.multiply(np.arctan2(np.sqrt(temp), np.sqrt(1-temp)),2)\n",
    "    return ret\n",
    "\n",
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "    \n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2 - theta_1) / 2 * np.pi / 180) ** 2\n",
    "           + np.cos(theta_1 * np.pi / 180) * np.cos(theta_2 * np.pi / 180) * np.sin((phi_2 - phi_1) / 2 * np.pi / 180) ** 2)\n",
    "    ret = np.multiply(np.arctan2(np.sqrt(temp), np.sqrt(1-temp)),2) \n",
    "    return ret.mean()\n",
    "\n",
    "def groupby_statistics(df):\n",
    "    gb = df.groupby(by='passenger_count').agg(\n",
    "      {\n",
    "        'fare_amount': ['mean', 'std'], \n",
    "        'tip_amount': ['mean', 'std']\n",
    "      }\n",
    "    )\n",
    "    return gb\n",
    "\n",
    "other = ks.DataFrame(groupby_statistics(koalas_data).to_pandas())\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "def join_count(df, other):\n",
    "    return len(df.merge(other.spark.hint(\"broadcast\"), left_index=True, right_index=True))\n",
    "\n",
    "def join_data(df, other):\n",
    "    ret = df.merge(other.spark.hint(\"broadcast\"), left_index=True, right_index=True)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file took: 0.29213380813598633 seconds\n",
      "count took: 0.23517465591430664 seconds\n",
      "count index length took: 0.1402449607849121 seconds\n",
      "mean took: 0.37973570823669434 seconds\n",
      "standard deviation took: 0.5381433963775635 seconds\n",
      "mean of columns addition took: 0.6047580242156982 seconds\n",
      "addition of columns took: 0.029071569442749023 seconds\n",
      "mean of columns multiplication took: 0.6008937358856201 seconds\n",
      "multiplication of columns took: 0.029781341552734375 seconds\n",
      "value counts took: 0.06059908866882324 seconds\n",
      "complex arithmetic ops took: 0.0 seconds\n",
      "mean of complex arithmetic ops took: 0.0 seconds\n",
      "groupby statistics took: 0.13081121444702148 seconds\n",
      "join count took: 18.390236377716064 seconds\n",
      "join took: 0.6384189128875732 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(read_file_parquet, df=None, benchmarks=koalas_benchmarks, name='read file')\n",
    "benchmark(count, df=koalas_data, benchmarks=koalas_benchmarks, name='count')\n",
    "benchmark(count_index_length, df=koalas_data, benchmarks=koalas_benchmarks, name='count index length')\n",
    "benchmark(mean, df=koalas_data, benchmarks=koalas_benchmarks, name='mean')\n",
    "benchmark(standard_deviation, df=koalas_data, benchmarks=koalas_benchmarks, name='standard deviation')\n",
    "benchmark(mean_of_sum, df=koalas_data, benchmarks=koalas_benchmarks, name='mean of columns addition')\n",
    "benchmark(sum_columns, df=koalas_data, benchmarks=koalas_benchmarks, name='addition of columns')\n",
    "benchmark(mean_of_product, df=koalas_data, benchmarks=koalas_benchmarks, name='mean of columns multiplication')\n",
    "benchmark(product_columns, df=koalas_data, benchmarks=koalas_benchmarks, name='multiplication of columns')\n",
    "benchmark(value_counts, df=koalas_data, benchmarks=koalas_benchmarks, name='value counts')\n",
    "benchmark(complicated_arithmetic_operation, df=koalas_data, benchmarks=koalas_benchmarks, name='complex arithmetic ops')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=koalas_data, benchmarks=koalas_benchmarks, name='mean of complex arithmetic ops')\n",
    "benchmark(groupby_statistics, df=koalas_data, benchmarks=koalas_benchmarks, name='groupby statistics')\n",
    "benchmark(join_count, koalas_data, benchmarks=koalas_benchmarks, name='join count', other=other)\n",
    "benchmark(join_data, koalas_data, benchmarks=koalas_benchmarks, name='join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations with filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_filter = (koalas_data.tip_amount >= 1) & (koalas_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "koalas_filtered = filter_data(koalas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 1.1444599628448486 seconds\n",
      "filtered count index length took: 1.096585750579834 seconds\n",
      "filtered mean took: 1.600801706314087 seconds\n",
      "filtered standard deviation took: 1.6130115985870361 seconds\n",
      "filtered mean of columns addition took: 1.2357845306396484 seconds\n",
      "filtered addition of columns took: 0.053076982498168945 seconds\n",
      "filtered mean of columns multiplication took: 1.6587965488433838 seconds\n",
      "filtered multiplication of columns took: 0.05656599998474121 seconds\n",
      "filtered mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered complex arithmetic ops took: 0.0 seconds\n",
      "filtered value counts took: 0.12258720397949219 seconds\n",
      "filtered groupby statistics took: 0.33182716369628906 seconds\n",
      "filtered join took: 1.6664671897888184 seconds\n",
      "filtered join count took: 36.37161159515381 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered count')\n",
    "benchmark(count_index_length, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered count index length')\n",
    "benchmark(mean, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=koalas_filtered, benchmarks=koalas_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=koalas_filtered, benchmarks=koalas_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other = ks.DataFrame(groupby_statistics(koalas_filtered).to_pandas())\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_data, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered join', other=other)\n",
    "benchmark(join_count, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered join count', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations with filtering and caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enforce caching: 4826147 rows of filtered data\n"
     ]
    }
   ],
   "source": [
    "koalas_filtered_cached = koalas_filtered.spark.cache()\n",
    "print(f'Enforce caching: {len(koalas_filtered_cached)} rows of filtered data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 1.6552259922027588 seconds\n",
      "filtered count index length took: 1.328669548034668 seconds\n",
      "filtered mean took: 1.2429137229919434 seconds\n",
      "filtered standard deviation took: 1.3116066455841064 seconds\n",
      "filtered mean of columns addition took: 1.3325088024139404 seconds\n",
      "filtered addition of columns took: 0.0428767204284668 seconds\n",
      "filtered mean of columns multiplication took: 1.2864344120025635 seconds\n",
      "filtered multiplication of columns took: 0.04985833168029785 seconds\n",
      "filtered mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered complex arithmetic ops took: 0.0 seconds\n",
      "filtered value counts took: 0.08036041259765625 seconds\n",
      "filtered groupby statistics took: 0.28113818168640137 seconds\n",
      "filtered join took: 1.1048705577850342 seconds\n",
      "filtered join count took: 15.40094518661499 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered count')\n",
    "benchmark(count_index_length, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered count index length')\n",
    "benchmark(mean, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=koalas_filtered, benchmarks=koalas_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=koalas_filtered, benchmarks=koalas_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other = ks.DataFrame(groupby_statistics(koalas_filtered).to_pandas())\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_data, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered join', other=other)\n",
    "benchmark(join_count, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered join count', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(memory_limit='8GB')\n",
    "client = Client(cluster)\n",
    "\n",
    "dask_data = dd.read_parquet(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_benchmarks = {\n",
    "    'duration': [],  # in seconds\n",
    "    'task': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return dd.read_parquet(\"../yellow_tripdata_2011-01.parquet\")\n",
    "\n",
    "def count(df=None):\n",
    "    return len(df)\n",
    "\n",
    "def count_index_length(df=None):\n",
    "    return len(df.index)\n",
    "\n",
    "def mean(df):\n",
    "    return df.fare_amount.mean().compute()\n",
    "\n",
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std().compute()\n",
    "\n",
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.tip_amount).mean().compute()\n",
    "\n",
    "def sum_columns(df):\n",
    "    return (df.fare_amount + df.tip_amount).compute()\n",
    "\n",
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.tip_amount).mean().compute()\n",
    "\n",
    "def product_columns(df):\n",
    "    return (df.fare_amount * df.tip_amount).compute()\n",
    "\n",
    "def value_counts(df):\n",
    "    return df.fare_amount.value_counts().compute()\n",
    "\n",
    "#   In the original experiment, the following two functions used the longitude and latitude values of the pickup and the dropout places.\n",
    "#   Since the datasets provided by NYC TLC Trip Record Data no longer have longitude and latitude values (only the pickup and dropout places IDs),\n",
    "# we used arbitrary longitude and latitude values. The goal of this experiment is to compare the computational cost of the calculations, hence\n",
    "# the values are not relevant.\n",
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret.mean()\n",
    "\n",
    "def complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret\n",
    "\n",
    "def groupby_statistics(df):\n",
    "    return df.groupby(by='passenger_count').agg({\n",
    "        'fare_amount': ['mean', 'std'],\n",
    "        'tip_amount': ['mean', 'std'] \n",
    "    })\n",
    "\n",
    "other = groupby_statistics(dask_data)\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "def join_count(df, other):\n",
    "    return len(dd.merge(df, other, left_index=True, right_index=True))\n",
    "\n",
    "def join_data(df, other):\n",
    "    return dd.merge(df, other, left_index=True, right_index=True).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file took: 0.034003257751464844 seconds\n",
      "count took: 0.358975887298584 seconds\n",
      "count index length took: 11.157870769500732 seconds\n",
      "mean took: 1.2081716060638428 seconds\n",
      "standard deviation took: 1.501474142074585 seconds\n",
      "mean of columns addition took: 2.0431041717529297 seconds\n",
      "addition of columns took: 2.578772783279419 seconds\n",
      "mean of columns multiplication took: 1.9190800189971924 seconds\n",
      "multiplication of columns took: 2.041478395462036 seconds\n",
      "value counts took: 0.6384503841400146 seconds\n",
      "mean of complex arithmetic ops took: 0.011385440826416016 seconds\n",
      "complex arithmetic ops took: 0.0 seconds\n",
      "groupby statistics took: 0.04836153984069824 seconds\n",
      "join count took: 15.860377311706543 seconds\n",
      "join took: 11.891459941864014 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(read_file_parquet, df=None, benchmarks=dask_benchmarks, name='read file')\n",
    "benchmark(count, df=dask_data, benchmarks=dask_benchmarks, name='count')\n",
    "benchmark(count_index_length, df=dask_data, benchmarks=dask_benchmarks, name='count index length')\n",
    "benchmark(mean, df=dask_data, benchmarks=dask_benchmarks, name='mean')\n",
    "benchmark(standard_deviation, df=dask_data, benchmarks=dask_benchmarks, name='standard deviation')\n",
    "benchmark(mean_of_sum, df=dask_data, benchmarks=dask_benchmarks, name='mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_data, benchmarks=dask_benchmarks, name='addition of columns')\n",
    "benchmark(mean_of_product, df=dask_data, benchmarks=dask_benchmarks, name='mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_data, benchmarks=dask_benchmarks, name='multiplication of columns')\n",
    "benchmark(value_counts, df=dask_data, benchmarks=dask_benchmarks, name='value counts')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=dask_data, benchmarks=dask_benchmarks, name='mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, df=dask_data, benchmarks=dask_benchmarks, name='complex arithmetic ops')\n",
    "benchmark(groupby_statistics, df=dask_data, benchmarks=dask_benchmarks, name='groupby statistics')\n",
    "benchmark(join_count, dask_data, benchmarks=dask_benchmarks, name='join count', other=other)\n",
    "benchmark(join_data, dask_data, benchmarks=dask_benchmarks, name='join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations with filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_filter = (dask_data.tip_amount >= 1) & (dask_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "dask_filtered = filter_data(dask_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 29.07993721961975 seconds\n",
      "filtered count index length took: 30.692441701889038 seconds\n",
      "filtered mean took: 20.941547870635986 seconds\n",
      "filtered standard deviation took: 17.086211442947388 seconds\n",
      "filtered mean of columns addition took: 17.045679807662964 seconds\n",
      "filtered addition of columns took: 13.413687229156494 seconds\n",
      "filtered mean of columns multiplication took: 10.60387110710144 seconds\n",
      "filtered multiplication of columns took: 11.928022623062134 seconds\n",
      "filtered mean of complex arithmetic ops took: 0.02328658103942871 seconds\n",
      "filtered complex arithmetic ops took: 0.0 seconds\n",
      "filtered value counts took: 12.697035789489746 seconds\n",
      "filtered groupby statistics took: 0.2729041576385498 seconds\n",
      "filtered join count took: 20.698776483535767 seconds\n",
      "filtered join took: 19.326754093170166 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, dask_filtered, benchmarks=dask_benchmarks, name='filtered count')\n",
    "benchmark(count_index_length, dask_filtered, benchmarks=dask_benchmarks, name='filtered count index length')\n",
    "benchmark(mean, dask_filtered, benchmarks=dask_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, dask_filtered, benchmarks=dask_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, dask_filtered, benchmarks=dask_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_filtered, benchmarks=dask_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, dask_filtered, benchmarks=dask_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_filtered, benchmarks=dask_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, dask_filtered, benchmarks=dask_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, dask_filtered, benchmarks=dask_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, dask_filtered, benchmarks=dask_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, dask_filtered, benchmarks=dask_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other = groupby_statistics(dask_filtered)\n",
    "other.columns = pd.Index([e[0] +'_'+ e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_count, dask_filtered, benchmarks=dask_benchmarks, name='filtered join count', other=other)\n",
    "benchmark(join_data, dask_filtered, benchmarks=dask_benchmarks, name='filtered join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations with filtering and caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting until all futures are finished\n",
      "All futures are finished\n"
     ]
    }
   ],
   "source": [
    "dask_filtered = client.persist(dask_filtered)\n",
    "\n",
    "from distributed import wait\n",
    "print('Waiting until all futures are finished')\n",
    "wait(dask_filtered)\n",
    "print('All futures are finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 0.05237889289855957 seconds\n",
      "filtered count index length took: 0.03733491897583008 seconds\n",
      "filtered mean took: 0.10164070129394531 seconds\n",
      "filtered standard deviation took: 0.3326871395111084 seconds\n",
      "filtered mean of columns addition took: 0.10929036140441895 seconds\n",
      "filtered addition of columns took: 0.6915786266326904 seconds\n",
      "filtered mean of columns multiplication took: 0.09549403190612793 seconds\n",
      "filtered multiplication of columns took: 0.6156878471374512 seconds\n",
      "filtered mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered complex arithmetic ops took: 0.0 seconds\n",
      "filtered value counts took: 0.10123610496520996 seconds\n",
      "filtered groupby statistics took: 0.03887438774108887 seconds\n",
      "filtered join count took: 0.8143243789672852 seconds\n",
      "filtered join took: 0.3657393455505371 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, dask_filtered, benchmarks=dask_benchmarks, name='filtered count')\n",
    "benchmark(count_index_length, dask_filtered, benchmarks=dask_benchmarks, name='filtered count index length')\n",
    "benchmark(mean, dask_filtered, benchmarks=dask_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, dask_filtered, benchmarks=dask_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, dask_filtered, benchmarks=dask_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_filtered, benchmarks=dask_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, dask_filtered, benchmarks=dask_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_filtered, benchmarks=dask_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, dask_filtered, benchmarks=dask_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, dask_filtered, benchmarks=dask_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, dask_filtered, benchmarks=dask_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, dask_filtered, benchmarks=dask_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other = groupby_statistics(dask_filtered)\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_count, dask_filtered, benchmarks=dask_benchmarks, name='filtered join count', other=other)\n",
    "benchmark(join_data, dask_filtered, benchmarks=dask_benchmarks, name='filtered join', other=other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:58707</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>29.80 GiB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:58707' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.restart()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
