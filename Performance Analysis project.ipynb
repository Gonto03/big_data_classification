{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Analysis Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work by: GonÃ§alo Dias and Vicente Bandeira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves as both an implementation and a report for the work proposed to us in the CDLE cadeira in IACD. In this report, we present the findings of our comprehensive analysis of machine learning performance using various Python libraries for big data processing. As the scale and complexity of data continue to grow, the choice of tools and libraries becomes critical in ensuring efficient and effective machine learning workflows. To address this, we executed a full Machine Learning Pipeline using several prominent big data libraries: PySpark, Dask, Modin, JobLib, Rapids and Koalas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeating the NYC taxi driver dataset study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.databricks.com/blog/2021/04/07/benchmark-koalas-pyspark-and-dask.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T20:09:01.610752Z",
     "start_time": "2024-06-02T20:09:01.563474Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T20:09:02.730105Z",
     "start_time": "2024-06-02T20:09:01.617736Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 1.1.5\n",
      "numpy version: 1.19.5\n",
      "koalas version: 1.7.0\n",
      "dask version: 2021.04.1\n"
     ]
    }
   ],
   "source": [
    "import databricks.koalas as ks\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print('pandas version: %s' % pd.__version__)\n",
    "print('numpy version: %s' % np.__version__)\n",
    "print('koalas version: %s' % ks.__version__)\n",
    "print('dask version: %s' % dask.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"../yellow_tripdata_2011-01.parquet\"\n",
    "koalas_data = ks.read_parquet(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 13464997\n",
      "\n",
      "    VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  improvement_surcharge  total_amount  congestion_surcharge  airport_fee\n",
      "0          2  2011-01-01 00:10:00   2011-01-01 00:12:00                4            0.0           1               None           145           145             1          2.9    0.5      0.5        0.28           0.0                    0.0          4.18                   NaN          NaN\n",
      "1          2  2011-01-01 00:04:00   2011-01-01 00:13:00                4            0.0           1               None           264           264             1          5.7    0.5      0.5        0.24           0.0                    0.0          6.94                   NaN          NaN\n",
      "2          2  2011-01-01 00:14:00   2011-01-01 00:16:00                4            0.0           1               None           264           264             1          2.9    0.5      0.5        1.11           0.0                    0.0          5.01                   NaN          NaN\n",
      "3          2  2011-01-01 00:04:00   2011-01-01 00:06:00                5            0.0           1               None           146           146             1          2.9    0.5      0.5        0.00           0.0                    0.0          3.90                   NaN          NaN\n",
      "4          2  2011-01-01 00:08:00   2011-01-01 00:08:00                5            0.0           1               None           146           146             1          2.5    0.5      0.5        0.11           0.0                    0.0          3.61                   NaN          NaN\n",
      "5          2  2011-01-01 00:23:00   2011-01-01 00:23:00                1            0.0           1               None           146           146             2          2.5    0.5      0.5        0.00           0.0                    0.0          3.50                   NaN          NaN\n",
      "6          2  2011-01-01 00:25:00   2011-01-01 00:25:00                1            0.0           1               None           146           146             2          2.5    0.5      0.5        0.00           0.0                    0.0          3.50                   NaN          NaN\n",
      "7          1  2011-01-01 00:58:10   2011-01-01 01:15:35                1            8.0           1                  N           138           256             2         20.1    0.5      0.5        0.00           0.0                    0.0         21.10                   NaN          NaN\n",
      "8          1  2011-01-01 00:23:27   2011-01-01 00:39:39                1            1.6           1                  N           170           237             2          9.3    0.5      0.5        0.00           0.0                    0.0         10.30                   NaN          NaN\n",
      "9          1  2011-01-01 00:42:08   2011-01-01 00:51:50                4            2.5           1                  N           237           170             2          8.1    0.5      0.5        0.00           0.0                    0.0          9.10                   NaN          NaN\n",
      "10         1  2011-01-01 00:53:36   2011-01-01 01:17:43                2            3.9           1                  N           170           239             1         14.9    0.5      0.5        2.38           0.0                    0.0         18.28                   NaN          NaN\n",
      "11         1  2011-01-01 00:37:47   2011-01-01 00:41:20                2            0.6           1                  N            90            90             2          4.1    0.5      0.5        0.00           0.0                    0.0          5.10                   NaN          NaN\n",
      "12         1  2011-01-01 00:42:49   2011-01-01 00:52:00                4            0.9           1                  N            90           186             2          6.5    0.5      0.5        0.00           0.0                    0.0          7.50                   NaN          NaN\n",
      "13         1  2011-01-01 00:56:28   2011-01-01 01:22:36                1            3.9           1                  Y            90           238             2         15.3    0.5      0.5        0.00           0.0                    0.0         16.30                   NaN          NaN\n",
      "14         1  2011-01-01 00:11:22   2011-01-01 00:14:36                2            0.7           1                  N           113            79             2          4.1    0.0      0.5        0.00           0.0                    0.0          4.60                   NaN          NaN\n",
      "15         1  2011-01-01 00:16:43   2011-01-01 00:24:39                2            1.9           1                  N            79           170             1          6.9    0.0      0.5        1.00           0.0                    0.0          8.40                   NaN          NaN\n",
      "16         1  2011-01-01 00:32:25   2011-01-01 00:48:46                2            3.3           1                  N           170           142             2         11.3    0.0      0.5        0.00           0.0                    0.0         11.80                   NaN          NaN\n",
      "17         1  2011-01-01 00:50:52   2011-01-01 01:25:47                2            5.3           1                  Y           142           112             2         20.1    0.0      0.5        0.00           4.8                    0.0         25.40                   NaN          NaN\n",
      "18         1  2011-01-01 00:20:22   2011-01-01 00:26:13                1            1.0           4                  N           114           249             2          5.3    0.5      0.5        0.00           0.0                    0.0          6.30                   NaN          NaN\n",
      "19         1  2011-01-01 00:28:45   2011-01-01 00:46:14                1            4.3           4                  N           249           143             2         13.7    0.5      0.5        0.00           0.0                    0.0         14.70                   NaN          NaN\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows:\", len(koalas_data))\n",
    "print()\n",
    "print(koalas_data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VendorID                          int64\n",
       "tpep_pickup_datetime     datetime64[ns]\n",
       "tpep_dropoff_datetime    datetime64[ns]\n",
       "passenger_count                   int64\n",
       "trip_distance                   float64\n",
       "RatecodeID                        int64\n",
       "store_and_fwd_flag               object\n",
       "PULocationID                      int64\n",
       "DOLocationID                      int64\n",
       "payment_type                      int64\n",
       "fare_amount                     float64\n",
       "extra                           float64\n",
       "mta_tax                         float64\n",
       "tip_amount                      float64\n",
       "tolls_amount                    float64\n",
       "improvement_surcharge           float64\n",
       "total_amount                    float64\n",
       "congestion_surcharge            float64\n",
       "airport_fee                     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koalas_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data is 35.84216914418919% of total data\n"
     ]
    }
   ],
   "source": [
    "expr_filter = (koalas_data['tip_amount'] >= 1) & (koalas_data['tip_amount'] <= 5)\n",
    " \n",
    "print(f'Filtered data is {len(koalas_data[expr_filter]) / len(koalas_data) * 100}% of total data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(f, df, benchmarks, name, **kwargs):\n",
    "    \"\"\"Benchmark the given function against the given DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f: function to benchmark\n",
    "    df: data frame\n",
    "    benchmarks: container for benchmark results\n",
    "    name: task name\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Duration (in seconds) of the given operation\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    ret = f(df, **kwargs)\n",
    "    benchmarks['duration'].append(time.time() - start_time)\n",
    "    benchmarks['task'].append(name)\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]} seconds\")\n",
    "\n",
    "def get_results(benchmarks):\n",
    "    \"\"\"Return a pandas DataFrame containing benchmark results.\"\"\"\n",
    "    return pd.DataFrame.from_dict(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"../yellow_tripdata_201\"+str(i)+\"-01.parquet\" for i in range(1,4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Koalas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we utilized the Koalas library for our experiments, trying it on Standard operations, operations with filtering and operations with filtering and caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "koalas_data = ks.read_parquet(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "koalas_benchmarks = {\n",
    "    'duration': [],     # in seconds\n",
    "    'task': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return ks.read_parquet(\"../yellow_tripdata_2011-01.parquet\")\n",
    "\n",
    "def count(df=None):\n",
    "    return len(df)\n",
    "\n",
    "def count_index_length(df=None):\n",
    "    return len(df.index)\n",
    "\n",
    "def mean(df):\n",
    "    return df.fare_amount.mean()\n",
    "\n",
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std()\n",
    "\n",
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.tip_amount).mean()\n",
    "\n",
    "def sum_columns(df):\n",
    "    x = df.fare_amount + df.tip_amount\n",
    "    return x\n",
    "\n",
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.tip_amount).mean()\n",
    "\n",
    "def product_columns(df):\n",
    "    x = df.fare_amount * df.tip_amount\n",
    "    return x\n",
    "\n",
    "def value_counts(df):\n",
    "    val_counts = df.fare_amount.value_counts()\n",
    "    return val_counts\n",
    "\n",
    "\n",
    "#   In the original experiment, the following two functions used the longitude and latitude values of the pickup and the dropout places.\n",
    "#   Since the datasets provided by NYC TLC Trip Record Data no longer have longitude and latitude values (only the pickup and dropout places IDs),\n",
    "# we used arbitrary longitude and latitude values. The goal of this experiment is to compare the computational cost of the calculations, hence\n",
    "# the values are not relevant.\n",
    "def complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "    \n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2 - theta_1) / 2 * np.pi / 180) ** 2\n",
    "           + np.cos(theta_1 * np.pi / 180) * np.cos(theta_2 * np.pi / 180) * np.sin((phi_2 - phi_1) / 2 * np.pi / 180) ** 2)\n",
    "    ret = np.multiply(np.arctan2(np.sqrt(temp), np.sqrt(1-temp)),2)\n",
    "    return ret\n",
    "\n",
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "    \n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2 - theta_1) / 2 * np.pi / 180) ** 2\n",
    "           + np.cos(theta_1 * np.pi / 180) * np.cos(theta_2 * np.pi / 180) * np.sin((phi_2 - phi_1) / 2 * np.pi / 180) ** 2)\n",
    "    ret = np.multiply(np.arctan2(np.sqrt(temp), np.sqrt(1-temp)),2) \n",
    "    return ret.mean()\n",
    "\n",
    "def groupby_statistics(df):\n",
    "    gb = df.groupby(by='passenger_count').agg(\n",
    "      {\n",
    "        'fare_amount': ['mean', 'std'], \n",
    "        'tip_amount': ['mean', 'std']\n",
    "      }\n",
    "    )\n",
    "    return gb\n",
    "\n",
    "other = ks.DataFrame(groupby_statistics(koalas_data).to_pandas())\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "def join_count(df, other):\n",
    "    return len(df.merge(other.spark.hint(\"broadcast\"), left_index=True, right_index=True))\n",
    "\n",
    "def join_data(df, other):\n",
    "    ret = df.merge(other.spark.hint(\"broadcast\"), left_index=True, right_index=True)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file took: 0.2836952209472656 seconds\n",
      "count took: 0.18452048301696777 seconds\n",
      "count index length took: 0.1263866424560547 seconds\n",
      "mean took: 0.4323129653930664 seconds\n",
      "standard deviation took: 0.564359188079834 seconds\n",
      "mean of columns addition took: 0.637066125869751 seconds\n",
      "addition of columns took: 0.03345060348510742 seconds\n",
      "mean of columns multiplication took: 0.6672379970550537 seconds\n",
      "multiplication of columns took: 0.024534225463867188 seconds\n",
      "value counts took: 0.06283807754516602 seconds\n",
      "complex arithmetic ops took: 0.002001523971557617 seconds\n",
      "mean of complex arithmetic ops took: 0.0 seconds\n",
      "groupby statistics took: 0.13896608352661133 seconds\n",
      "join count took: 20.747236728668213 seconds\n",
      "join took: 0.6573514938354492 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(read_file_parquet, df=None, benchmarks=koalas_benchmarks, name='read file')\n",
    "benchmark(count, df=koalas_data, benchmarks=koalas_benchmarks, name='count')\n",
    "benchmark(count_index_length, df=koalas_data, benchmarks=koalas_benchmarks, name='count index length')\n",
    "benchmark(mean, df=koalas_data, benchmarks=koalas_benchmarks, name='mean')\n",
    "benchmark(standard_deviation, df=koalas_data, benchmarks=koalas_benchmarks, name='standard deviation')\n",
    "benchmark(mean_of_sum, df=koalas_data, benchmarks=koalas_benchmarks, name='mean of columns addition')\n",
    "benchmark(sum_columns, df=koalas_data, benchmarks=koalas_benchmarks, name='addition of columns')\n",
    "benchmark(mean_of_product, df=koalas_data, benchmarks=koalas_benchmarks, name='mean of columns multiplication')\n",
    "benchmark(product_columns, df=koalas_data, benchmarks=koalas_benchmarks, name='multiplication of columns')\n",
    "benchmark(value_counts, df=koalas_data, benchmarks=koalas_benchmarks, name='value counts')\n",
    "benchmark(complicated_arithmetic_operation, df=koalas_data, benchmarks=koalas_benchmarks, name='complex arithmetic ops')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=koalas_data, benchmarks=koalas_benchmarks, name='mean of complex arithmetic ops')\n",
    "benchmark(groupby_statistics, df=koalas_data, benchmarks=koalas_benchmarks, name='groupby statistics')\n",
    "benchmark(join_count, koalas_data, benchmarks=koalas_benchmarks, name='join count', other=other)\n",
    "benchmark(join_data, koalas_data, benchmarks=koalas_benchmarks, name='join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations with filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_filter = (koalas_data.tip_amount >= 1) & (koalas_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "koalas_filtered = filter_data(koalas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 0.7095263004302979 seconds\n",
      "filtered count index length took: 0.506678581237793 seconds\n",
      "filtered mean took: 0.8717646598815918 seconds\n",
      "filtered standard deviation took: 1.026289701461792 seconds\n",
      "filtered mean of columns addition took: 1.3728179931640625 seconds\n",
      "filtered addition of columns took: 0.03542971611022949 seconds\n",
      "filtered mean of columns multiplication took: 0.9695253372192383 seconds\n",
      "filtered multiplication of columns took: 0.024727582931518555 seconds\n",
      "filtered mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered complex arithmetic ops took: 0.0 seconds\n",
      "filtered value counts took: 0.05472874641418457 seconds\n",
      "filtered groupby statistics took: 0.1295621395111084 seconds\n",
      "filtered join took: 0.5105862617492676 seconds\n",
      "filtered join count took: 21.96998405456543 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered count')\n",
    "benchmark(count_index_length, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered count index length')\n",
    "benchmark(mean, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=koalas_filtered, benchmarks=koalas_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=koalas_filtered, benchmarks=koalas_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other = ks.DataFrame(groupby_statistics(koalas_filtered).to_pandas())\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_data, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered join', other=other)\n",
    "benchmark(join_count, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered join count', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations with filtering and caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enforce caching: 4826147 rows of filtered data\n"
     ]
    }
   ],
   "source": [
    "koalas_filtered_cached = koalas_filtered.spark.cache()\n",
    "print(f'Enforce caching: {len(koalas_filtered_cached)} rows of filtered data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 1.5313990116119385 seconds\n",
      "filtered count index length took: 1.484971046447754 seconds\n",
      "filtered mean took: 1.7788598537445068 seconds\n",
      "filtered standard deviation took: 1.7384052276611328 seconds\n",
      "filtered mean of columns addition took: 1.8392524719238281 seconds\n",
      "filtered addition of columns took: 0.03993058204650879 seconds\n",
      "filtered mean of columns multiplication took: 1.8636271953582764 seconds\n",
      "filtered multiplication of columns took: 0.03415489196777344 seconds\n",
      "filtered mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered complex arithmetic ops took: 0.0 seconds\n",
      "filtered value counts took: 0.06516480445861816 seconds\n",
      "filtered groupby statistics took: 0.16971755027770996 seconds\n",
      "filtered join took: 0.6518526077270508 seconds\n",
      "filtered join count took: 9.557752847671509 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached count')\n",
    "benchmark(count_index_length, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached count index length')\n",
    "benchmark(mean, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached mean')\n",
    "benchmark(standard_deviation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached standard deviation')\n",
    "benchmark(mean_of_sum, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered and cached mean of columns addition')\n",
    "benchmark(sum_columns, df=koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached addition of columns')\n",
    "benchmark(mean_of_product, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered and cached mean of columns multiplication')\n",
    "benchmark(product_columns, df=koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached complex arithmetic ops')\n",
    "benchmark(value_counts, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered and cached value counts')\n",
    "benchmark(groupby_statistics, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached groupby statistics')\n",
    "\n",
    "other = ks.DataFrame(groupby_statistics(koalas_filtered).to_pandas())\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_data, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached join', other=other)\n",
    "benchmark(join_count, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached join count', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeated this experiment with Dask, on the same 3 sets of operations: standard, with filtering and with filtering and caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(memory_limit='8GB')\n",
    "client = Client(cluster)\n",
    "\n",
    "dask_data = dd.read_parquet(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_benchmarks = {\n",
    "    'duration': [],  # in seconds\n",
    "    'task': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return dd.read_parquet(\"../yellow_tripdata_2011-01.parquet\")\n",
    "\n",
    "def count(df=None):\n",
    "    return len(df)\n",
    "\n",
    "def count_index_length(df=None):\n",
    "    return len(df.index)\n",
    "\n",
    "def mean(df):\n",
    "    return df.fare_amount.mean().compute()\n",
    "\n",
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std().compute()\n",
    "\n",
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.tip_amount).mean().compute()\n",
    "\n",
    "def sum_columns(df):\n",
    "    return (df.fare_amount + df.tip_amount).compute()\n",
    "\n",
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.tip_amount).mean().compute()\n",
    "\n",
    "def product_columns(df):\n",
    "    return (df.fare_amount * df.tip_amount).compute()\n",
    "\n",
    "def value_counts(df):\n",
    "    return df.fare_amount.value_counts().compute()\n",
    "\n",
    "#   In the original experiment, the following two functions used the longitude and latitude values of the pickup and the dropout places.\n",
    "#   Since the datasets provided by NYC TLC Trip Record Data no longer have longitude and latitude values (only the pickup and dropout places IDs),\n",
    "# we used arbitrary longitude and latitude values. The goal of this experiment is to compare the computational cost of the calculations, hence\n",
    "# the values are not relevant.\n",
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret.mean()\n",
    "\n",
    "def complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret\n",
    "\n",
    "def groupby_statistics(df):\n",
    "    return df.groupby(by='passenger_count').agg({\n",
    "        'fare_amount': ['mean', 'std'],\n",
    "        'tip_amount': ['mean', 'std'] \n",
    "    })\n",
    "\n",
    "other = groupby_statistics(dask_data)\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "def join_count(df, other):\n",
    "    return len(dd.merge(df, other, left_index=True, right_index=True))\n",
    "\n",
    "def join_data(df, other):\n",
    "    return dd.merge(df, other, left_index=True, right_index=True).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file took: 0.04585576057434082 seconds\n",
      "count took: 1.3291070461273193 seconds\n",
      "count index length took: 43.1181058883667 seconds\n",
      "mean took: 1.2414824962615967 seconds\n",
      "standard deviation took: 3.0880119800567627 seconds\n",
      "mean of columns addition took: 3.27643084526062 seconds\n",
      "addition of columns took: 2.332578420639038 seconds\n",
      "mean of columns multiplication took: 2.07999324798584 seconds\n",
      "multiplication of columns took: 2.26393985748291 seconds\n",
      "value counts took: 1.158517599105835 seconds\n",
      "mean of complex arithmetic ops took: 0.0 seconds\n",
      "complex arithmetic ops took: 0.0 seconds\n",
      "groupby statistics took: 0.06505417823791504 seconds\n",
      "join count took: 30.098060846328735 seconds\n",
      "join took: 18.45586347579956 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(read_file_parquet, df=None, benchmarks=dask_benchmarks, name='read file')\n",
    "benchmark(count, df=dask_data, benchmarks=dask_benchmarks, name='count')\n",
    "benchmark(count_index_length, df=dask_data, benchmarks=dask_benchmarks, name='count index length')\n",
    "benchmark(mean, df=dask_data, benchmarks=dask_benchmarks, name='mean')\n",
    "benchmark(standard_deviation, df=dask_data, benchmarks=dask_benchmarks, name='standard deviation')\n",
    "benchmark(mean_of_sum, df=dask_data, benchmarks=dask_benchmarks, name='mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_data, benchmarks=dask_benchmarks, name='addition of columns')\n",
    "benchmark(mean_of_product, df=dask_data, benchmarks=dask_benchmarks, name='mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_data, benchmarks=dask_benchmarks, name='multiplication of columns')\n",
    "benchmark(value_counts, df=dask_data, benchmarks=dask_benchmarks, name='value counts')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=dask_data, benchmarks=dask_benchmarks, name='mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, df=dask_data, benchmarks=dask_benchmarks, name='complex arithmetic ops')\n",
    "benchmark(groupby_statistics, df=dask_data, benchmarks=dask_benchmarks, name='groupby statistics')\n",
    "benchmark(join_count, dask_data, benchmarks=dask_benchmarks, name='join count', other=other)\n",
    "benchmark(join_data, dask_data, benchmarks=dask_benchmarks, name='join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations with filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_filter = (dask_data.tip_amount >= 1) & (dask_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "dask_filtered = filter_data(dask_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 17.790358304977417 seconds\n",
      "filtered count index length took: 16.307208776474 seconds\n",
      "filtered mean took: 16.096224069595337 seconds\n",
      "filtered standard deviation took: 17.178290605545044 seconds\n",
      "filtered mean of columns addition took: 19.5005521774292 seconds\n",
      "filtered addition of columns took: 17.45691418647766 seconds\n",
      "filtered mean of columns multiplication took: 16.436156272888184 seconds\n",
      "filtered multiplication of columns took: 17.223954439163208 seconds\n",
      "filtered mean of complex arithmetic ops took: 0.007777690887451172 seconds\n",
      "filtered complex arithmetic ops took: 0.0 seconds\n",
      "filtered value counts took: 17.735454320907593 seconds\n",
      "filtered groupby statistics took: 0.09854698181152344 seconds\n",
      "filtered join count took: 17.671422958374023 seconds\n",
      "filtered join took: 17.869524240493774 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, dask_filtered, benchmarks=dask_benchmarks, name='filtered count')\n",
    "benchmark(count_index_length, dask_filtered, benchmarks=dask_benchmarks, name='filtered count index length')\n",
    "benchmark(mean, dask_filtered, benchmarks=dask_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, dask_filtered, benchmarks=dask_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, dask_filtered, benchmarks=dask_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_filtered, benchmarks=dask_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, dask_filtered, benchmarks=dask_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_filtered, benchmarks=dask_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, dask_filtered, benchmarks=dask_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, dask_filtered, benchmarks=dask_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, dask_filtered, benchmarks=dask_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, dask_filtered, benchmarks=dask_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other = groupby_statistics(dask_filtered)\n",
    "other.columns = pd.Index([e[0] +'_'+ e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_count, dask_filtered, benchmarks=dask_benchmarks, name='filtered join count', other=other)\n",
    "benchmark(join_data, dask_filtered, benchmarks=dask_benchmarks, name='filtered join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations with filtering and caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting until all futures are finished\n",
      "All futures are finished\n"
     ]
    }
   ],
   "source": [
    "dask_filtered = client.persist(dask_filtered)\n",
    "\n",
    "from distributed import wait\n",
    "print('Waiting until all futures are finished')\n",
    "wait(dask_filtered)\n",
    "print('All futures are finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 0.1582040786743164 seconds\n",
      "filtered count index length took: 0.036712646484375 seconds\n",
      "filtered mean took: 0.14176106452941895 seconds\n",
      "filtered standard deviation took: 0.41079092025756836 seconds\n",
      "filtered mean of columns addition took: 0.14326071739196777 seconds\n",
      "filtered addition of columns took: 1.1182324886322021 seconds\n",
      "filtered mean of columns multiplication took: 0.20514416694641113 seconds\n",
      "filtered multiplication of columns took: 0.9939935207366943 seconds\n",
      "filtered mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered complex arithmetic ops took: 0.0 seconds\n",
      "filtered value counts took: 0.14677929878234863 seconds\n",
      "filtered groupby statistics took: 0.07665109634399414 seconds\n",
      "filtered join count took: 1.5119473934173584 seconds\n",
      "filtered join took: 0.6601705551147461 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached count')\n",
    "benchmark(count_index_length, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached count index length')\n",
    "benchmark(mean, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached mean')\n",
    "benchmark(standard_deviation, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached standard deviation')\n",
    "benchmark(mean_of_sum, dask_filtered, benchmarks=dask_benchmarks, name ='filtered and cached mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached addition of columns')\n",
    "benchmark(mean_of_product, dask_filtered, benchmarks=dask_benchmarks, name ='filtered and cached mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached complex arithmetic ops')\n",
    "benchmark(value_counts, dask_filtered, benchmarks=dask_benchmarks, name ='filtered and cached value counts')\n",
    "benchmark(groupby_statistics, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached groupby statistics')\n",
    "\n",
    "other = groupby_statistics(dask_filtered)\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_count, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached join count', other=other)\n",
    "benchmark(join_data, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached join', other=other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:59597</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>29.80 GiB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:59597' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'duration': [0.2836952209472656, 0.18452048301696777, 0.1263866424560547, 0.4323129653930664, 0.564359188079834, 0.637066125869751, 0.03345060348510742, 0.6672379970550537, 0.024534225463867188, 0.06283807754516602, 0.002001523971557617, 0.0, 0.13896608352661133, 20.747236728668213, 0.6573514938354492, 0.7095263004302979, 0.506678581237793, 0.8717646598815918, 1.026289701461792, 1.3728179931640625, 0.03542971611022949, 0.9695253372192383, 0.024727582931518555, 0.0, 0.0, 0.05472874641418457, 0.1295621395111084, 0.5105862617492676, 21.96998405456543, 1.5313990116119385, 1.484971046447754, 1.7788598537445068, 1.7384052276611328, 1.8392524719238281, 0.03993058204650879, 1.8636271953582764, 0.03415489196777344, 0.0, 0.0, 0.06516480445861816, 0.16971755027770996, 0.6518526077270508, 9.557752847671509], 'task': ['read file', 'count', 'count index length', 'mean', 'standard deviation', 'mean of columns addition', 'addition of columns', 'mean of columns multiplication', 'multiplication of columns', 'value counts', 'complex arithmetic ops', 'mean of complex arithmetic ops', 'groupby statistics', 'join count', 'join', 'filtered count', 'filtered count index length', 'filtered mean', 'filtered standard deviation', 'filtered mean of columns addition', 'filtered addition of columns', 'filtered mean of columns multiplication', 'filtered multiplication of columns', 'filtered mean of complex arithmetic ops', 'filtered complex arithmetic ops', 'filtered value counts', 'filtered groupby statistics', 'filtered join', 'filtered join count', 'filtered count', 'filtered count index length', 'filtered mean', 'filtered standard deviation', 'filtered mean of columns addition', 'filtered addition of columns', 'filtered mean of columns multiplication', 'filtered multiplication of columns', 'filtered mean of complex arithmetic ops', 'filtered complex arithmetic ops', 'filtered value counts', 'filtered groupby statistics', 'filtered join', 'filtered join count']}\n",
      "{'duration': [0.04585576057434082, 1.3291070461273193, 43.1181058883667, 1.2414824962615967, 3.0880119800567627, 3.27643084526062, 2.332578420639038, 2.07999324798584, 2.26393985748291, 1.158517599105835, 0.0, 0.0, 0.06505417823791504, 30.098060846328735, 18.45586347579956, 17.790358304977417, 16.307208776474, 16.096224069595337, 17.178290605545044, 19.5005521774292, 17.45691418647766, 16.436156272888184, 17.223954439163208, 0.007777690887451172, 0.0, 17.735454320907593, 0.09854698181152344, 17.671422958374023, 17.869524240493774, 0.1582040786743164, 0.036712646484375, 0.14176106452941895, 0.41079092025756836, 0.14326071739196777, 1.1182324886322021, 0.20514416694641113, 0.9939935207366943, 0.0, 0.0, 0.14677929878234863, 0.07665109634399414, 1.5119473934173584, 0.6601705551147461], 'task': ['read file', 'count', 'count index length', 'mean', 'standard deviation', 'mean of columns addition', 'addition of columns', 'mean of columns multiplication', 'multiplication of columns', 'value counts', 'mean of complex arithmetic ops', 'complex arithmetic ops', 'groupby statistics', 'join count', 'join', 'filtered count', 'filtered count index length', 'filtered mean', 'filtered standard deviation', 'filtered mean of columns addition', 'filtered addition of columns', 'filtered mean of columns multiplication', 'filtered multiplication of columns', 'filtered mean of complex arithmetic ops', 'filtered complex arithmetic ops', 'filtered value counts', 'filtered groupby statistics', 'filtered join count', 'filtered join', 'filtered count', 'filtered count index length', 'filtered mean', 'filtered standard deviation', 'filtered mean of columns addition', 'filtered addition of columns', 'filtered mean of columns multiplication', 'filtered multiplication of columns', 'filtered mean of complex arithmetic ops', 'filtered complex arithmetic ops', 'filtered value counts', 'filtered groupby statistics', 'filtered join count', 'filtered join']}\n"
     ]
    }
   ],
   "source": [
    "print(koalas_benchmarks)\n",
    "print(dask_benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task read file: Dask performs 6.186686632039224x better than Koalas\n",
      "Task count: Koalas performs 7.203032554413484x better than Dask\n",
      "Task count index length: Koalas performs 341.1603081659448x better than Dask\n",
      "Task mean: Koalas performs 2.8717216360439695x better than Dask\n",
      "Task standard deviation: Koalas performs 5.47171383983906x better than Dask\n",
      "Task mean of columns addition: Koalas performs 5.142999623135654x better than Dask\n",
      "Task addition of columns: Koalas performs 69.73202805376972x better than Dask\n",
      "Task mean of columns multiplication: Koalas performs 3.1173183439285155x better than Dask\n",
      "Task multiplication of columns: Koalas performs 92.27680167923502x better than Dask\n",
      "Task value counts: Koalas performs 18.43655382794181x better than Dask\n",
      "Task mean of complex arithmetic ops: Dask ran this task in less time than the measurable threshold while Koalas took 0.002001523971557617 seconds.\n",
      "Task complex arithmetic ops: both Dask and Koalas ran this task in less time than the measurable threshold.\n",
      "Task groupby statistics: Dask performs 2.136159233591222x better than Koalas\n",
      "Task join count: Koalas performs 1.450702146023123x better than Dask\n",
      "Task join: Koalas performs 28.076095740066126x better than Dask\n",
      "Task filtered count: Koalas performs 25.07357133088416x better than Dask\n",
      "Task filtered count index length: Koalas performs 32.18452364146955x better than Dask\n",
      "Task filtered mean: Koalas performs 18.463955709998178x better than Dask\n",
      "Task filtered standard deviation: Koalas performs 16.738247086643476x better than Dask\n",
      "Task filtered mean of columns addition: Koalas performs 14.204761501183741x better than Dask\n",
      "Task filtered addition of columns: Koalas performs 492.7195615162547x better than Dask\n",
      "Task filtered mean of columns multiplication: Koalas performs 16.952786731731884x better than Dask\n",
      "Task filtered multiplication of columns: Koalas performs 696.5482427806971x better than Dask\n",
      "Task filtered mean of complex arithmetic ops: Koalas ran this task in less time than the measurable threshold while Dask took 0.007777690887451172 seconds.\n",
      "Task filtered complex arithmetic ops: both Dask and Koalas ran this task in less time than the measurable threshold.\n",
      "Task filtered value counts: Koalas performs 324.0610370770511x better than Dask\n",
      "Task filtered groupby statistics: Dask performs 1.314724582422049x better than Koalas\n",
      "Task filtered join count: Koalas performs 34.61006353330339x better than Dask\n",
      "Task filtered join: Dask performs 1.2294666471746172x better than Koalas\n",
      "Task filtered count: Dask performs 9.679895894242536x better than Koalas\n",
      "Task filtered count index length: Dask performs 40.448488154613464x better than Koalas\n",
      "Task filtered mean: Dask performs 12.54829638624327x better than Koalas\n",
      "Task filtered standard deviation: Dask performs 4.231849200978304x better than Koalas\n",
      "Task filtered mean of columns addition: Dask performs 12.838498266705942x better than Koalas\n",
      "Task filtered addition of columns: Koalas performs 28.004412440814182x better than Dask\n",
      "Task filtered mean of columns multiplication: Dask performs 9.084475679218816x better than Koalas\n",
      "Task filtered multiplication of columns: Koalas performs 29.10252275646395x better than Dask\n",
      "Task filtered mean of complex arithmetic ops: both Dask and Koalas ran this task in less time than the measurable threshold.\n",
      "Task filtered complex arithmetic ops: both Dask and Koalas ran this task in less time than the measurable threshold.\n",
      "Task filtered value counts: Koalas performs 2.2524321219372094x better than Dask\n",
      "Task filtered groupby statistics: Dask performs 2.214156853230813x better than Koalas\n",
      "Task filtered join count: Koalas performs 2.3194620616605x better than Dask\n",
      "Task filtered join: Dask performs 14.477702426474094x better than Koalas\n"
     ]
    }
   ],
   "source": [
    "dask_koalas_duration_ratio = []\n",
    "i=0\n",
    "for task in dask_benchmarks['task']:\n",
    "    dask_duration_i = dask_benchmarks['duration'][i]\n",
    "    koalas_duration_i = koalas_benchmarks['duration'][i]\n",
    "    if dask_duration_i == 0 and koalas_duration_i == 0:\n",
    "        print(f\"Task {task}: both Dask and Koalas ran this task in less time than the measurable threshold.\")\n",
    "        i+=1\n",
    "        continue\n",
    "    if dask_duration_i == 0:\n",
    "        print(f\"Task {task}: Dask ran this task in less time than the measurable threshold while Koalas took {koalas_duration_i} seconds.\")\n",
    "        i+=1\n",
    "        continue\n",
    "    if koalas_duration_i == 0:       \n",
    "        print(f\"Task {task}: Koalas ran this task in less time than the measurable threshold while Dask took {dask_duration_i} seconds.\")\n",
    "        i+=1\n",
    "        continue\n",
    "    ratio = dask_duration_i / koalas_duration_i\n",
    "    dask_koalas_duration_ratio.append(ratio)\n",
    "    if ratio >= 1:\n",
    "        print(f\"Task {task}: Koalas performs {ratio}x better than Dask\")\n",
    "    else:\n",
    "        print(f\"Task {task}: Dask performs {1/ratio}x better than Koalas\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
