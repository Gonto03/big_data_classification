{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeating the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.databricks.com/blog/2021/04/07/benchmark-koalas-pyspark-and-dask.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 2.0.3\n",
      "numpy version: 1.24.3\n",
      "koalas version: 0.32.0\n",
      "dask version: 2023.6.0\n"
     ]
    }
   ],
   "source": [
    "import databricks.koalas as ks\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print('pandas version: %s' % pd.__version__)\n",
    "print('numpy version: %s' % np.__version__)\n",
    "print('koalas version: %s' % ks.__version__)\n",
    "print('dask version: %s' % dask.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"../yellow_tripdata_2011-01.parquet\"\n",
    "koalas_data = ks.read_parquet(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 13464997\n",
      "\n",
      "    VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  improvement_surcharge  total_amount  congestion_surcharge  airport_fee\n",
      "0          2  2011-01-01 00:10:00   2011-01-01 00:12:00                4            0.0           1               None           145           145             1          2.9    0.5      0.5        0.28           0.0                    0.0          4.18                   NaN          NaN\n",
      "1          2  2011-01-01 00:04:00   2011-01-01 00:13:00                4            0.0           1               None           264           264             1          5.7    0.5      0.5        0.24           0.0                    0.0          6.94                   NaN          NaN\n",
      "2          2  2011-01-01 00:14:00   2011-01-01 00:16:00                4            0.0           1               None           264           264             1          2.9    0.5      0.5        1.11           0.0                    0.0          5.01                   NaN          NaN\n",
      "3          2  2011-01-01 00:04:00   2011-01-01 00:06:00                5            0.0           1               None           146           146             1          2.9    0.5      0.5        0.00           0.0                    0.0          3.90                   NaN          NaN\n",
      "4          2  2011-01-01 00:08:00   2011-01-01 00:08:00                5            0.0           1               None           146           146             1          2.5    0.5      0.5        0.11           0.0                    0.0          3.61                   NaN          NaN\n",
      "5          2  2011-01-01 00:23:00   2011-01-01 00:23:00                1            0.0           1               None           146           146             2          2.5    0.5      0.5        0.00           0.0                    0.0          3.50                   NaN          NaN\n",
      "6          2  2011-01-01 00:25:00   2011-01-01 00:25:00                1            0.0           1               None           146           146             2          2.5    0.5      0.5        0.00           0.0                    0.0          3.50                   NaN          NaN\n",
      "7          1  2011-01-01 00:58:10   2011-01-01 01:15:35                1            8.0           1                  N           138           256             2         20.1    0.5      0.5        0.00           0.0                    0.0         21.10                   NaN          NaN\n",
      "8          1  2011-01-01 00:23:27   2011-01-01 00:39:39                1            1.6           1                  N           170           237             2          9.3    0.5      0.5        0.00           0.0                    0.0         10.30                   NaN          NaN\n",
      "9          1  2011-01-01 00:42:08   2011-01-01 00:51:50                4            2.5           1                  N           237           170             2          8.1    0.5      0.5        0.00           0.0                    0.0          9.10                   NaN          NaN\n",
      "10         1  2011-01-01 00:53:36   2011-01-01 01:17:43                2            3.9           1                  N           170           239             1         14.9    0.5      0.5        2.38           0.0                    0.0         18.28                   NaN          NaN\n",
      "11         1  2011-01-01 00:37:47   2011-01-01 00:41:20                2            0.6           1                  N            90            90             2          4.1    0.5      0.5        0.00           0.0                    0.0          5.10                   NaN          NaN\n",
      "12         1  2011-01-01 00:42:49   2011-01-01 00:52:00                4            0.9           1                  N            90           186             2          6.5    0.5      0.5        0.00           0.0                    0.0          7.50                   NaN          NaN\n",
      "13         1  2011-01-01 00:56:28   2011-01-01 01:22:36                1            3.9           1                  Y            90           238             2         15.3    0.5      0.5        0.00           0.0                    0.0         16.30                   NaN          NaN\n",
      "14         1  2011-01-01 00:11:22   2011-01-01 00:14:36                2            0.7           1                  N           113            79             2          4.1    0.0      0.5        0.00           0.0                    0.0          4.60                   NaN          NaN\n",
      "15         1  2011-01-01 00:16:43   2011-01-01 00:24:39                2            1.9           1                  N            79           170             1          6.9    0.0      0.5        1.00           0.0                    0.0          8.40                   NaN          NaN\n",
      "16         1  2011-01-01 00:32:25   2011-01-01 00:48:46                2            3.3           1                  N           170           142             2         11.3    0.0      0.5        0.00           0.0                    0.0         11.80                   NaN          NaN\n",
      "17         1  2011-01-01 00:50:52   2011-01-01 01:25:47                2            5.3           1                  Y           142           112             2         20.1    0.0      0.5        0.00           4.8                    0.0         25.40                   NaN          NaN\n",
      "18         1  2011-01-01 00:20:22   2011-01-01 00:26:13                1            1.0           4                  N           114           249             2          5.3    0.5      0.5        0.00           0.0                    0.0          6.30                   NaN          NaN\n",
      "19         1  2011-01-01 00:28:45   2011-01-01 00:46:14                1            4.3           4                  N           249           143             2         13.7    0.5      0.5        0.00           0.0                    0.0         14.70                   NaN          NaN\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows:\", len(koalas_data))\n",
    "print()\n",
    "print(koalas_data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VendorID                          int64\n",
       "tpep_pickup_datetime     datetime64[ns]\n",
       "tpep_dropoff_datetime    datetime64[ns]\n",
       "passenger_count                   int64\n",
       "trip_distance                   float64\n",
       "RatecodeID                        int64\n",
       "store_and_fwd_flag               object\n",
       "PULocationID                      int64\n",
       "DOLocationID                      int64\n",
       "payment_type                      int64\n",
       "fare_amount                     float64\n",
       "extra                           float64\n",
       "mta_tax                         float64\n",
       "tip_amount                      float64\n",
       "tolls_amount                    float64\n",
       "improvement_surcharge           float64\n",
       "total_amount                    float64\n",
       "congestion_surcharge            float64\n",
       "airport_fee                     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koalas_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the benchmark, filtered data is 35.84216914418919% of total data\n"
     ]
    }
   ],
   "source": [
    "expr_filter = (koalas_data['tip_amount'] >= 1) & (koalas_data['tip_amount'] <= 5)\n",
    " \n",
    "print(f'Filtered data is {len(koalas_data[expr_filter]) / len(koalas_data) * 100}% of total data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(f, df, benchmarks, name, **kwargs):\n",
    "    \"\"\"Benchmark the given function against the given DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f: function to benchmark\n",
    "    df: data frame\n",
    "    benchmarks: container for benchmark results\n",
    "    name: task name\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Duration (in seconds) of the given operation\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    ret = f(df, **kwargs)\n",
    "    benchmarks['duration'].append(time.time() - start_time)\n",
    "    benchmarks['task'].append(name)\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]} seconds\")\n",
    "    return benchmarks['duration'][-1]\n",
    "\n",
    "\n",
    "def get_results(benchmarks):\n",
    "    \"\"\"Return a pandas DataFrame containing benchmark results.\"\"\"\n",
    "    return pd.DataFrame.from_dict(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../yellow_tripdata_2011-01.parquet', '../yellow_tripdata_2012-01.parquet', '../yellow_tripdata_2013-01.parquet']\n"
     ]
    }
   ],
   "source": [
    "paths = [\"../yellow_tripdata_201\"+str(i)+\"-01.parquet\" for i in range(1,4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Koalas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "koalas_data = ks.read_parquet(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "koalas_benchmarks = {\n",
    "    'duration': [],     # in seconds\n",
    "    'task': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_parquet(url):\n",
    "    return ks.read_parquet(url, index_col='index')\n",
    "\n",
    "def count(df=None):\n",
    "    return len(df)\n",
    "\n",
    "def count_index_length(df=None):\n",
    "    return len(df.index)\n",
    "\n",
    "def mean(df):\n",
    "    return df.fare_amount.mean()\n",
    "\n",
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std()\n",
    "\n",
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.tip_amount).mean()\n",
    "\n",
    "def sum_columns(df):\n",
    "    x = df.fare_amount + df.tip_amount\n",
    "    x.to_pandas()\n",
    "    return x\n",
    "\n",
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.tip_amount).mean()\n",
    "\n",
    "def product_columns(df):\n",
    "    x = df.fare_amount * df.tip_amount\n",
    "    x.to_pandas()\n",
    "    return x\n",
    "\n",
    "def value_counts(df):\n",
    "    val_counts = df.fare_amount.value_counts()\n",
    "    val_counts.to_pandas()\n",
    "    return val_counts\n",
    "\n",
    "\n",
    "#   In the original experiment, the following two functions used the longitude and latitude values of the pickup and the dropout places.\n",
    "#   Since the datasets provided by NYC TLC Trip Record Data no longer have longitude and latitude values (only the pickup and dropout places IDs),\n",
    "# we used arbitrary longitude and latitude values. The goal of this experiment is to compare the computational cost of the calculations, hence\n",
    "# the values are not relevant.\n",
    "def complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "    \n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2 - theta_1) / 2 * np.pi / 180) ** 2\n",
    "           + np.cos(theta_1 * np.pi / 180) * np.cos(theta_2 * np.pi / 180) * np.sin((phi_2 - phi_1) / 2 * np.pi / 180) ** 2)\n",
    "    ret = np.multiply(np.arctan2(np.sqrt(temp), np.sqrt(1-temp)),2)\n",
    "    ret.to_pandas()\n",
    "    return ret\n",
    "\n",
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "    \n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2 - theta_1) / 2 * np.pi / 180) ** 2\n",
    "           + np.cos(theta_1 * np.pi / 180) * np.cos(theta_2 * np.pi / 180) * np.sin((phi_2 - phi_1) / 2 * np.pi / 180) ** 2)\n",
    "    ret = np.multiply(np.arctan2(np.sqrt(temp), np.sqrt(1-temp)),2) \n",
    "    return ret.mean()\n",
    "\n",
    "\n",
    "def groupby_statistics(df):\n",
    "    gb = df.groupby(by='passenger_count').agg(\n",
    "      {\n",
    "        'fare_amount': ['mean', 'std'], \n",
    "        'tip_amount': ['mean', 'std']\n",
    "      }\n",
    "    )\n",
    "    gb.to_pandas()\n",
    "    return gb\n",
    "\n",
    "other = ks.DataFrame(groupby_statistics(koalas_data).to_pandas())\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "def join_count(df, other):\n",
    "    return len(df.merge(other.spark.hint(\"broadcast\"), left_index=True, right_index=True))\n",
    "\n",
    "def join_data(df, other):\n",
    "    ret = df.merge(other.spark.hint(\"broadcast\"), left_index=True, right_index=True)\n",
    "    ret.to_pandas()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations with filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_filter = (koalas_data.tip_amount >= 1) & (koalas_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "koalas_filtered = filter_data(koalas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations with filtering and caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "koalas_filtered_cached = koalas_filtered.spark.cache()\n",
    "print(f'Enforce caching: {len(koalas_filtered_cached)} rows of filtered data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "\n",
    "dask_data = dd.read_parquet(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_benchmarks = {\n",
    "    'duration': [],  # in seconds\n",
    "    'task': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return dd.read_parquet('/dbfs/FileStore/ks_taxi_parquet', index='index')\n",
    "\n",
    "def count(df=None):\n",
    "    return len(df)\n",
    "\n",
    "def count_index_length(df=None):\n",
    "    return len(df.index)\n",
    "\n",
    "def mean(df):\n",
    "    return df.fare_amount.mean().compute()\n",
    "\n",
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std().compute()\n",
    "\n",
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.tip_amount).mean().compute()\n",
    "\n",
    "def sum_columns(df):\n",
    "    return (df.fare_amount + df.tip_amount).compute()\n",
    "\n",
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.tip_amount).mean().compute()\n",
    "\n",
    "def product_columns(df):\n",
    "    return (df.fare_amount * df.tip_amount).compute()\n",
    "\n",
    "def value_counts(df):\n",
    "    return df.fare_amount.value_counts().compute()\n",
    "\n",
    "#   In the original experiment, the following two functions used the longitude and latitude values of the pickup and the dropout places.\n",
    "#   Since the datasets provided by NYC TLC Trip Record Data no longer have longitude and latitude values (only the pickup and dropout places IDs),\n",
    "# we used arbitrary longitude and latitude values. The goal of this experiment is to compare the computational cost of the calculations, hence\n",
    "# the values are not relevant.\n",
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret.mean().compute()\n",
    "\n",
    "def complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret.compute()\n",
    "\n",
    "\n",
    "def groupby_statistics(df):\n",
    "    return df.groupby(by='passenger_count').agg(\n",
    "      {\n",
    "        'fare_amount': ['mean', 'std'], \n",
    "        'tip_amount': ['mean', 'std']\n",
    "      }\n",
    "    ).compute()\n",
    "\n",
    "other = groupby_statistics(dask_data)\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "def join_count(df, other):\n",
    "    return len(dd.merge(df, other, left_index=True, right_index=True))\n",
    "\n",
    "def join_data(df, other):\n",
    "    return dd.merge(df, other, left_index=True, right_index=True).compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
